{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtIuEUE1RSBD0NHG3TJYKB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc0adbd5b704422fa165e795ef3fe825": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_07a1f6438c1c44f7ac794de09754d48e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[36mCrawling:\u001b[0m https://docs.crawl4ai.com/blog/releases/0.4.1/ (depth=2) \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m   0/2500\u001b[0m \u001b[33m0:05:19\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Crawling:</span> https://docs.crawl4ai.com/blog/releases/0.4.1/ (depth=2) <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">   0/2500</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:05:19</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "07a1f6438c1c44f7ac794de09754d48e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9e21979781d443a85632ddddac5d79d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_700c085ecdff4d19a78a8c62aa9d6441",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[36mCrawling:\u001b[0m https://docs.crawl4ai.com/blog/articles/dockerize_hooks/ (depth=0) \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m   0/2500\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Crawling:</span> https://docs.crawl4ai.com/blog/articles/dockerize_hooks/ (depth=0) <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">   0/2500</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:04</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "700c085ecdff4d19a78a8c62aa9d6441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498cf006295d40cdb316e9d820831910": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a94039ca39d4414ca4fbe82a7ab9e119",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[36mCrawling:\u001b[0m https://docs.crawl4ai.com/blog/releases/v0.4.3b1/ (depth=0) \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m   0/2500\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Crawling:</span> https://docs.crawl4ai.com/blog/releases/v0.4.3b1/ (depth=0) <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">   0/2500</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:06</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "a94039ca39d4414ca4fbe82a7ab9e119": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9311109b957a4464919351cf0bf7faa7": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c7cdf81f667e48c4a0b5c5b236605ff3",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[36mCrawling:\u001b[0m https://crawl4ai.com/extraction/chunking/ (depth=1) \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m   0/2500\u001b[0m \u001b[33m0:01:21\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Crawling:</span> https://crawl4ai.com/extraction/chunking/ (depth=1) <span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">   0/2500</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:01:21</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c7cdf81f667e48c4a0b5c5b236605ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wyattowalsh/sitedumper/blob/main/SiteDumper_(Crawl4AI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc0adbd5b704422fa165e795ef3fe825",
            "07a1f6438c1c44f7ac794de09754d48e",
            "c9e21979781d443a85632ddddac5d79d",
            "700c085ecdff4d19a78a8c62aa9d6441",
            "498cf006295d40cdb316e9d820831910",
            "a94039ca39d4414ca4fbe82a7ab9e119",
            "9311109b957a4464919351cf0bf7faa7",
            "c7cdf81f667e48c4a0b5c5b236605ff3"
          ]
        },
        "id": "2RNNbVfz4lH8",
        "outputId": "afbfa227-3363-4717-fc14-fe809570551d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libgtk-4.so.1                                    ║\n",
            "║     libgraphene-1.0.so.0                             ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:865:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:963:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:952:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "✅ Installation step complete!\n",
            "✅ Crawl4AI modules imported and logging set up!\n",
            "\n",
            "20:18:43 | ========== BUILDING CONFIGS ==========\n",
            "20:18:43 | Using NoExtractionStrategy.\n",
            "20:18:44 | Failed to parse sitemap at https://crawl4ai.com/sitemap.xml: not well-formed (invalid token): line 1, column 0\n",
            "20:18:44 | Default sitemap not found. Trying fallback sitemap: https://crawl4ai.com/mkdocs/sitemap.xml\n",
            "20:18:44 | Sitemap discovered 40 links.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc0adbd5b704422fa165e795ef3fe825"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:18:51 | Found 36 new links from https://docs.crawl4ai.com/\n",
            "20:18:59 | Found 35 new links from https://docs.crawl4ai.com/core/fit-markdown/\n",
            "20:19:09 | Found 35 new links from https://docs.crawl4ai.com/core/crawler-result/\n",
            "20:19:15 | Found 35 new links from https://docs.crawl4ai.com/advanced/ssl-certificate/\n",
            "20:19:26 | Found 35 new links from https://docs.crawl4ai.com/core/browser-crawler-config/\n",
            "20:19:37 | Found 35 new links from https://docs.crawl4ai.com/advanced/advanced-features/\n",
            "20:19:46 | Found 35 new links from https://docs.crawl4ai.com/core/page-interaction/\n",
            "20:19:58 | Found 35 new links from https://docs.crawl4ai.com/core/quickstart/\n",
            "20:20:02 | Found 35 new links from https://docs.crawl4ai.com/advanced/crawl-dispatcher/\n",
            "20:20:08 | Found 35 new links from https://docs.crawl4ai.com/advanced/file-downloading/\n",
            "20:20:22 | Found 35 new links from https://docs.crawl4ai.com/extraction/no-llm-strategies/\n",
            "20:20:29 | Found 35 new links from https://docs.crawl4ai.com/advanced/hooks-auth/\n",
            "20:20:41 | Found 35 new links from https://docs.crawl4ai.com/api/crawl-result/\n",
            "20:20:47 | Found 35 new links from https://docs.crawl4ai.com/core/installation/\n",
            "20:20:53 | Found 35 new links from https://docs.crawl4ai.com/advanced/proxy-security/\n",
            "20:21:03 | Found 35 new links from https://docs.crawl4ai.com/extraction/llm-strategies/\n",
            "20:21:11 | Found 35 new links from https://docs.crawl4ai.com/extraction/clustring-strategies/\n",
            "20:21:17 | Found 35 new links from https://docs.crawl4ai.com/extraction/chunking/\n",
            "20:21:26 | Found 35 new links from https://docs.crawl4ai.com/advanced/session-management/\n",
            "20:21:37 | Found 35 new links from https://docs.crawl4ai.com/advanced/multi-url-crawling/\n",
            "20:21:46 | Found 35 new links from https://docs.crawl4ai.com/api/arun/\n",
            "20:21:55 | Found 35 new links from https://docs.crawl4ai.com/core/link-media/\n",
            "20:22:05 | Found 35 new links from https://docs.crawl4ai.com/api/parameters/\n",
            "20:22:10 | Found 38 new links from https://docs.crawl4ai.com/blog/\n",
            "20:22:10 | Page crawl failed: Unexpected error in _crawl_web at line 1354 in _crawl_web (../usr/local/lib/python3.11/dist-packages/crawl4ai/async_crawler_strategy.py):\n",
            "Error: Failed on navigating ACS-GOTO:\n",
            "Page.goto: net::ERR_NAME_NOT_RESOLVED at https://old.docs.crawl4ai.com/\n",
            "Call log:\n",
            "  - navigating to \"https://old.docs.crawl4ai.com/\", waiting until \"domcontentloaded\"\n",
            "\n",
            "\n",
            "Code context:\n",
            "1349                       response = await page.goto(\n",
            "1350                           url, wait_until=config.wait_until, timeout=config.page_timeout\n",
            "1351                       )\n",
            "1352                       redirected_url = page.url\n",
            "1353                   except Error as e:\n",
            "1354 →                     raise RuntimeError(f\"Failed on navigating ACS-GOTO:\\n{str(e)}\")\n",
            "1355   \n",
            "1356                   await self.execute_hook(\n",
            "1357                       \"after_goto\", page, context=context, url=url, response=response, config=config\n",
            "1358                   )\n",
            "1359   \n",
            "20:22:17 | Found 35 new links from https://docs.crawl4ai.com/advanced/identity-based-crawling/\n",
            "20:22:24 | Found 35 new links from https://docs.crawl4ai.com/advanced/lazy-loading/\n",
            "20:22:31 | Found 35 new links from https://docs.crawl4ai.com/api/arun_many/\n",
            "20:22:48 | Found 35 new links from https://docs.crawl4ai.com/core/docker-deploymeny/\n",
            "20:22:54 | Found 35 new links from https://docs.crawl4ai.com/core/simple-crawling/\n",
            "20:23:02 | Found 35 new links from https://docs.crawl4ai.com/api/strategies/\n",
            "20:23:07 | Found 35 new links from https://docs.crawl4ai.com/core/cache-modes/\n",
            "20:23:17 | Found 35 new links from https://docs.crawl4ai.com/api/async-webcrawler/\n",
            "20:23:30 | Found 35 new links from https://docs.crawl4ai.com/core/markdown-generation/\n",
            "20:23:41 | Found 35 new links from https://docs.crawl4ai.com/core/content-selection/\n",
            "20:23:48 | Found 35 new links from https://docs.crawl4ai.com/core/local-files/\n",
            "20:23:53 | Found 36 new links from https://docs.crawl4ai.com/blog/releases/0.4.2/\n",
            "20:23:58 | Found 36 new links from https://docs.crawl4ai.com/blog/releases/0.4.0/\n",
            "20:24:04 | Found 36 new links from https://docs.crawl4ai.com/blog/releases/0.4.1/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9e21979781d443a85632ddddac5d79d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:24:09 | Found 36 new links from https://docs.crawl4ai.com/blog/articles/dockerize_hooks/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "498cf006295d40cdb316e9d820831910"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:24:16 | Found 36 new links from https://docs.crawl4ai.com/blog/releases/v0.4.3b1/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9311109b957a4464919351cf0bf7faa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:24:22 | Found 38 new links from https://crawl4ai.com\n",
            "20:24:28 | Found 37 new links from https://crawl4ai.com/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:25:37 | Writing output in json format...\n",
            "20:25:37 | Wrote /content/crawl4ai_output_json.json (563061 bytes)\n",
            "20:25:37 | Writing output in md format...\n",
            "20:25:37 | Wrote /content/crawl4ai_output_md.md (544481 bytes)\n",
            "20:25:37 | Writing output in html format...\n",
            "20:25:37 | Wrote /content/crawl4ai_output_html.html (544787 bytes)\n",
            "20:25:37 | Writing output in txt format...\n",
            "20:25:37 | Wrote /content/crawl4ai_output_txt.txt (543940 bytes)\n",
            "20:25:37 | Initiating automatic download of output files...\n",
            "20:25:37 | Downloading /content/crawl4ai_output_json.json...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24f5505d-0573-4c4a-815b-0dcc8a112a5e\", \"crawl4ai_output_json.json\", 563061)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:25:37 | Downloading /content/crawl4ai_output_md.md...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_51b279be-9f4c-4388-acf7-b9b7637936d5\", \"crawl4ai_output_md.md\", 544481)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:25:37 | Downloading /content/crawl4ai_output_html.html...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d619e4b1-da08-4d4f-8ec9-e25ae0c1ca02\", \"crawl4ai_output_html.html\", 544787)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:25:37 | Downloading /content/crawl4ai_output_txt.txt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_784552cb-dae3-461b-9547-2be26fa34f54\", \"crawl4ai_output_txt.txt\", 543940)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title **Enhanced Crawl4AI Colab Notebook**\n",
        "# This single-cell notebook script installs and sets up Crawl4AI with an advanced BFS crawler.\n",
        "# It includes enhanced sitemap discovery (with fallback), robust link extraction, improved logging & progress,\n",
        "# output cleaning for downstream LLM ingestion, conditional file splitting with automatic local download,\n",
        "# optional Google Drive integration, reverse crawling options, subdomain allowance, and JS-heavy page support.\n",
        "\n",
        "import sys, os, asyncio, json, re, collections, math, fnmatch, traceback, xml.etree.ElementTree as ET\n",
        "\n",
        "# Install/update packages and Playwright, plus extras (loguru, rich, nest_asyncio, tenacity, pydantic, aiohttp)\n",
        "try:\n",
        "    !{sys.executable} -m pip install --upgrade --quiet \"crawl4ai[all]\"\n",
        "    !{sys.executable} -m playwright install\n",
        "    !{sys.executable} -m pip install --upgrade --quiet nest_asyncio loguru rich tenacity pydantic google-colab aiohttp\n",
        "    print(\"✅ Installation step complete!\")\n",
        "except Exception as e:\n",
        "    print(\"❌ Installation failed:\", e)\n",
        "    raise e\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Optional: Mount Google Drive (set enable_google_drive=True to mount)\n",
        "enable_google_drive = False  #@param {type:\"boolean\"}\n",
        "drive_folder = \"Crawl4AI_Results\"  #@param {type:\"string\"}\n",
        "if enable_google_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    output_dir = os.path.join(\"/content/drive/My Drive\", drive_folder)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(\"✅ Google Drive mounted. Output directory:\", output_dir)\n",
        "else:\n",
        "    output_dir = os.getcwd()  # save locally in current directory\n",
        "\n",
        "# New: Option to automatically download output files to the local machine (only applicable in Colab)\n",
        "auto_download_files = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# =============================================================================\n",
        "#                IMPORTS FROM CRAWL4AI & PYTHON LIBS\n",
        "# =============================================================================\n",
        "from loguru import logger\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress, BarColumn, MofNCompleteColumn, TimeElapsedColumn\n",
        "from pydantic import BaseModel, Field\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "\n",
        "from crawl4ai import (\n",
        "    AsyncWebCrawler,\n",
        "    CacheMode,\n",
        "    BrowserConfig,\n",
        "    CrawlerRunConfig\n",
        ")\n",
        "try:\n",
        "    from crawl4ai.async_configs import (\n",
        "        MemoryAdaptiveDispatcher,\n",
        "        SemaphoreDispatcher,\n",
        "        RateLimiter,\n",
        "        CrawlerMonitor,\n",
        "        DisplayMode\n",
        "    )\n",
        "except ImportError:\n",
        "    MemoryAdaptiveDispatcher = SemaphoreDispatcher = RateLimiter = CrawlerMonitor = DisplayMode = None\n",
        "\n",
        "from crawl4ai.extraction_strategy import (\n",
        "    NoExtractionStrategy,\n",
        "    JsonCssExtractionStrategy,\n",
        "    JsonXPathExtractionStrategy,\n",
        "    LLMExtractionStrategy,\n",
        "    CosineStrategy\n",
        ")\n",
        "from crawl4ai.chunking_strategy import (\n",
        "    RegexChunking,\n",
        "    SlidingWindowChunking,\n",
        "    OverlappingWindowChunking\n",
        ")\n",
        "from crawl4ai.content_filter_strategy import (\n",
        "    LLMContentFilter,\n",
        "    PruningContentFilter\n",
        ")\n",
        "from crawl4ai.markdown_generation_strategy import (\n",
        "    DefaultMarkdownGenerator\n",
        ")\n",
        "\n",
        "console = Console()\n",
        "logger.remove()  # Remove default logger\n",
        "logger.add(sys.stdout, format=\"<green>{time:HH:mm:ss}</green> | <level>{message}</level>\", level=\"INFO\")\n",
        "print(\"✅ Crawl4AI modules imported and logging set up!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "#                   COLAB FORM PARAMETERS\n",
        "# =============================================================================\n",
        "# 1) Basic Settings\n",
        "url_to_crawl = \"https://crawl4ai.com\"  #@param {type:\"string\"}\n",
        "use_headless = True  #@param {type:\"boolean\"}\n",
        "verbose_browser = False  #@param {type:\"boolean\"}\n",
        "browser_type_choice = \"chromium\"  #@param [\"chromium\",\"firefox\",\"webkit\"]\n",
        "\n",
        "# Cache Mode Options\n",
        "cache_mode_choice = \"BYPASS\"  #@param [\"ENABLED\", \"DISABLED\", \"READ_ONLY\", \"WRITE_ONLY\", \"BYPASS\"]\n",
        "\n",
        "# Magic (Anti-Bot) Mode\n",
        "magic_mode = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# 2) Extraction Strategy Settings\n",
        "extraction_strategy_choice = \"NoExtractionStrategy\"  #@param [\"NoExtractionStrategy\",\"JsonCssExtractionStrategy\",\"JsonXPathExtractionStrategy\",\"LLMExtractionStrategy\",\"CosineStrategy\"]\n",
        "llm_instruction_text = \"Please summarize the page content.\"  #@param {type:\"string\"}\n",
        "llm_provider = \"openai/gpt-3.5-turbo\"  #@param {type:\"string\"}\n",
        "llm_api_token = \"\"  #@param {type:\"string\"}\n",
        "llm_input_format = \"markdown\"  #@param [\"markdown\",\"html\",\"fit_markdown\"]\n",
        "\n",
        "cosine_semantic_filter = \"Key points\"  #@param {type:\"string\"}\n",
        "cosine_word_threshold = 10  #@param {type:\"integer\"}\n",
        "cosine_sim_threshold = 0.3  #@param {type:\"number\"}\n",
        "cosine_top_k = 3  #@param {type:\"integer\"}\n",
        "\n",
        "jsoncss_base_selector = \".article\"  #@param {type:\"string\"}\n",
        "jsonxpath_base_selector = \"//div[@class='article']\"  #@param {type:\"string\"}\n",
        "\n",
        "# 3) Additional Options (Screenshots, PDF, Lazy Loading, etc.)\n",
        "want_pdf = False  #@param {type:\"boolean\"}\n",
        "want_screenshot = False  #@param {type:\"boolean\"}\n",
        "storage_state_path = \"\"  #@param {type:\"string\"}\n",
        "# Use scan_full_page to support JS-heavy pages (e.g., infinite scroll)\n",
        "scan_full_page = True  #@param {type:\"boolean\"}\n",
        "scroll_delay = 0.3  #@param {type:\"number\"}\n",
        "wait_for_images = False  #@param {type:\"boolean\"}\n",
        "base_word_count_threshold = 5  #@param {type:\"integer\"}\n",
        "\n",
        "# 4) Concurrency & Dispatcher Settings\n",
        "dispatcher_type = \"None\"  #@param [\"None\",\"MemoryAdaptiveDispatcher\",\"SemaphoreDispatcher\"] {allow-input: true}\n",
        "max_session_permit = 10  #@param {type:\"integer\"}\n",
        "semaphore_count = 5  #@param {type:\"integer\"}\n",
        "enable_rate_limit = False  #@param {type:\"boolean\"}\n",
        "rate_base_delay_min = 1.0  #@param {type:\"number\"}\n",
        "rate_base_delay_max = 2.0  #@param {type:\"number\"}\n",
        "rate_max_retries = 2  #@param {type:\"integer\"}\n",
        "display_monitor_mode = \"NONE\"  #@param [\"NONE\",\"SIMPLE\",\"DETAILED\"]\n",
        "\n",
        "# 5) BFS / Multi-Page Settings\n",
        "enable_bfs = True  #@param {type:\"boolean\"}\n",
        "max_depth = 7  #@param {type:\"integer\"}\n",
        "max_pages = 2500  #@param {type:\"integer\"}\n",
        "# Option to limit crawling strictly to the domain or allow subdomains.\n",
        "limit_to_domain = True  #@param {type:\"boolean\"}\n",
        "allow_subdomains = True  #@param {type:\"boolean\"}  # If True, subdomains (e.g. docs.crawl4ai.com, crawl4ai.com/mkdocs) are allowed.\n",
        "include_patterns_csv = \"\"  #@param {type:\"string\"}\n",
        "exclude_patterns_csv = \"\"  #@param {type:\"string\"}\n",
        "enable_sitemap_parsing = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# 6) Proxy Settings\n",
        "enable_proxy = False  #@param {type:\"boolean\"}\n",
        "proxy_url = \"\"  #@param {type:\"string\"}\n",
        "proxy_username = \"\"  #@param {type:\"string\"}\n",
        "proxy_password = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "# 7) Hooks & Session ID\n",
        "session_id_value = \"\"  #@param {type:\"string\"}\n",
        "enable_hooks = False  #@param {type:\"boolean\"}\n",
        "hook_selector = \".content\"  #@param {type:\"string\"}\n",
        "\n",
        "# 8) Output Options & File Splitting\n",
        "output_json = True  #@param {type:\"boolean\"}\n",
        "output_md   = True  #@param {type:\"boolean\"}\n",
        "output_html = True  #@param {type:\"boolean\"}\n",
        "output_txt  = True  #@param {type:\"boolean\"}\n",
        "max_output_size_mb = 10  #@param {type:\"integer\"}\n",
        "output_base_filename = \"crawl4ai_output\"  #@param {type:\"string\"}\n",
        "\n",
        "# 9) Execution\n",
        "execute_crawl = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# 10) Reverse/Backwards Crawling Options:\n",
        "# If disable_reverse_crawling is True, only URLs that start with the original seed URL will be followed.\n",
        "disable_reverse_crawling = False  #@param {type:\"boolean\"}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "#       HOOK FUNCTIONS (optional)\n",
        "# -----------------------------------------------------------------------------\n",
        "async def custom_before_goto(page, context, url, **kwargs):\n",
        "    logger.info(f\"[HOOK] Before navigating to {url}\")\n",
        "    return page\n",
        "\n",
        "async def custom_after_goto(page, context, url, response, **kwargs):\n",
        "    logger.info(f\"[HOOK] After loading {url}\")\n",
        "    if hook_selector.strip():\n",
        "        try:\n",
        "            await page.wait_for_selector(hook_selector.strip(), timeout=3000)\n",
        "            logger.info(f\"[HOOK] Selector {hook_selector.strip()} found.\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"[HOOK] Selector {hook_selector.strip()} not found: {e}\")\n",
        "    return page\n",
        "\n",
        "# =============================================================================\n",
        "#       UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "import urllib.parse\n",
        "def get_domain(url: str) -> str:\n",
        "    try:\n",
        "        parsed = urllib.parse.urlparse(url)\n",
        "        return parsed.netloc.lower()\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def same_domain(url: str, domain: str, allow_subdomains: bool=False) -> bool:\n",
        "    d = get_domain(url)\n",
        "    if allow_subdomains:\n",
        "        return d.endswith(domain)\n",
        "    else:\n",
        "        return d == domain\n",
        "\n",
        "def remove_fragment(url: str) -> str:\n",
        "    parsed = urllib.parse.urlparse(url)\n",
        "    return urllib.parse.urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, parsed.query, \"\"))\n",
        "\n",
        "def match_patterns(url: str, patterns: list) -> bool:\n",
        "    for pat in patterns:\n",
        "        pat = pat.strip()\n",
        "        if pat and fnmatch.fnmatch(url, pat):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Enhanced sitemap parser with fallback URL\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10), retry=retry_if_exception_type(Exception))\n",
        "async def fetch_text(url: str) -> str:\n",
        "    import aiohttp\n",
        "    async with aiohttp.ClientSession() as sess:\n",
        "        async with sess.get(url, timeout=15) as resp:\n",
        "            return await resp.text()\n",
        "\n",
        "async def parse_sitemap(url: str) -> list:\n",
        "    results = []\n",
        "    try:\n",
        "        text = await fetch_text(url)\n",
        "        root = ET.fromstring(text)\n",
        "        ns = \"\"\n",
        "        if root.tag.startswith(\"{\"):\n",
        "            ns = root.tag.split(\"}\")[0].strip(\"{\")\n",
        "        if \"sitemapindex\" in root.tag.lower():\n",
        "            for sitemap in root.findall(f\".//{{{ns}}}sitemap\"):\n",
        "                loc = sitemap.find(f\".//{{{ns}}}loc\")\n",
        "                if loc is not None and loc.text:\n",
        "                    sub_url = loc.text.strip()\n",
        "                    results.extend(await parse_sitemap(sub_url))\n",
        "        elif \"urlset\" in root.tag.lower():\n",
        "            for url_tag in root.findall(f\".//{{{ns}}}url\"):\n",
        "                loc = url_tag.find(f\".//{{{ns}}}loc\")\n",
        "                if loc is not None and loc.text:\n",
        "                    results.append(loc.text.strip())\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to parse sitemap at {url}: {e}\")\n",
        "    return results\n",
        "\n",
        "async def enhanced_sitemap_parser(domain: str) -> list:\n",
        "    # Try default sitemap at /sitemap.xml, then fallback to /mkdocs/sitemap.xml\n",
        "    default = domain.rstrip(\"/\") + \"/sitemap.xml\"\n",
        "    fallback = domain.rstrip(\"/\") + \"/mkdocs/sitemap.xml\"\n",
        "    links = await parse_sitemap(default)\n",
        "    if not links:\n",
        "        logger.info(f\"Default sitemap not found. Trying fallback sitemap: {fallback}\")\n",
        "        links = await parse_sitemap(fallback)\n",
        "    return links\n",
        "\n",
        "# =============================================================================\n",
        "#       BFS Crawler Implementation (No Redundant Route Tree)\n",
        "# =============================================================================\n",
        "class BFSCollector:\n",
        "    def __init__(self, start_url: str, max_pages: int):\n",
        "        self.start_url = remove_fragment(start_url)\n",
        "        self.max_pages = max_pages\n",
        "        self.visited = set([self.start_url])\n",
        "        self.contents = {}  # url -> page text\n",
        "\n",
        "    def is_visited(self, url: str) -> bool:\n",
        "        return url in self.visited\n",
        "\n",
        "    def mark_visited(self, url: str):\n",
        "        self.visited.add(url)\n",
        "\n",
        "    def set_result(self, url: str, success: bool, status_code: int, error: str = \"\", text: str = \"\"):\n",
        "        if text:\n",
        "            self.contents[url] = text\n",
        "\n",
        "    def get_metadata(self, config_summary: dict) -> dict:\n",
        "        return {\n",
        "            \"start_url\": self.start_url,\n",
        "            \"total_visited\": len(self.visited),\n",
        "            \"visited\": list(self.visited),\n",
        "            \"config_summary\": config_summary\n",
        "        }\n",
        "\n",
        "async def gather_links(result, domain: str, limit_to_dom: bool, includes: list, excludes: list, allow_subdomains: bool) -> list:\n",
        "    if not result.success or not result.links:\n",
        "        return []\n",
        "    combined = result.links.get(\"internal\", []) + result.links.get(\"external\", [])\n",
        "    out = []\n",
        "    for linfo in combined:\n",
        "        href = linfo.get(\"href\", \"\").strip()\n",
        "        if not href:\n",
        "            continue\n",
        "        href = remove_fragment(href)\n",
        "        if limit_to_dom and not same_domain(href, domain, allow_subdomains):\n",
        "            continue\n",
        "        if includes and not match_patterns(href, includes):\n",
        "            continue\n",
        "        if excludes and match_patterns(href, excludes):\n",
        "            continue\n",
        "        out.append(href)\n",
        "    return list(set(out))\n",
        "\n",
        "@retry(stop=stop_after_attempt(2), wait=wait_exponential(min=1, max=5), retry=retry_if_exception_type(Exception))\n",
        "async def single_page_crawl(url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig, session_id: str = None):\n",
        "    return await crawler.arun(url, config=config, session_id=session_id)\n",
        "\n",
        "async def do_bfs(start_url: str, domain: str, collector: BFSCollector,\n",
        "                 crawler: AsyncWebCrawler, run_config: CrawlerRunConfig,\n",
        "                 includes: list, excludes: list, limit_to_dom: bool, allow_subdomains: bool,\n",
        "                 max_depth: int, max_pages: int, session_id: str, disable_reverse_crawling: bool):\n",
        "    queue = collections.deque()\n",
        "    queue.append((start_url, 0))\n",
        "    with Progress(\n",
        "        \"[progress.description]{task.description}\",\n",
        "        BarColumn(),\n",
        "        MofNCompleteColumn(),\n",
        "        TimeElapsedColumn(),\n",
        "        console=console\n",
        "    ) as progress:\n",
        "        task_id = progress.add_task(\"BFS Crawling\", total=max_pages)\n",
        "        crawled_count = 0\n",
        "        while queue and crawled_count < max_pages:\n",
        "            cur_url, depth = queue.popleft()\n",
        "            progress.update(task_id, description=f\"[cyan]Crawling:[/] {cur_url} (depth={depth})\")\n",
        "            crawled_count += 1\n",
        "            # Dynamically update total based on number of visited URLs.\n",
        "            progress.update(task_id, total=max(max_pages, len(collector.visited)))\n",
        "            try:\n",
        "                result = await single_page_crawl(cur_url, crawler, run_config, session_id=session_id)\n",
        "                if not result.success:\n",
        "                    logger.warning(f\"Page crawl failed: {result.error_message}\")\n",
        "                    collector.set_result(cur_url, False, result.status_code, error=result.error_message)\n",
        "                    continue\n",
        "                text = result.markdown or \"\"\n",
        "                # Clean output for downstream ingestion: remove stray tokens, fix formatting.\n",
        "                text = re.sub(r\"<https?://[^>]+>\", lambda m: m.group(0).replace(\"<\", \"\").replace(\">\", \"\"), text)\n",
        "                collector.set_result(cur_url, True, result.status_code, text=text)\n",
        "                if depth < max_depth and 200 <= result.status_code < 300:\n",
        "                    new_links = await gather_links(result, domain, limit_to_dom, includes, excludes, allow_subdomains)\n",
        "                    logger.info(f\"Found {len(new_links)} new links from {cur_url}\")\n",
        "                    for child_url in new_links:\n",
        "                        # Respect reverse crawling option: if disabled, only add if link starts with the seed URL.\n",
        "                        if disable_reverse_crawling and not child_url.startswith(url_to_crawl):\n",
        "                            continue\n",
        "                        if not collector.is_visited(child_url):\n",
        "                            collector.mark_visited(child_url)\n",
        "                            queue.append((child_url, depth + 1))\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Exception at {cur_url}: {e}\")\n",
        "                collector.set_result(cur_url, False, 0, error=str(e))\n",
        "        progress.stop()\n",
        "\n",
        "# =============================================================================\n",
        "#       OUTPUT FORMATTING & FILE SPLITTING (with conditional part suffix)\n",
        "# =============================================================================\n",
        "def chunk_large_text(text: str, max_bytes: int) -> list:\n",
        "    data = text.encode(\"utf-8\")\n",
        "    if len(data) <= max_bytes:\n",
        "        return [text]\n",
        "    parts = []\n",
        "    start = 0\n",
        "    while start < len(data):\n",
        "        end = min(start + max_bytes, len(data))\n",
        "        chunk = data[start:end].decode(\"utf-8\", \"ignore\")\n",
        "        parts.append(chunk)\n",
        "        start = end\n",
        "    return parts\n",
        "\n",
        "def build_output_string(collector: BFSCollector, meta: dict, fmt: str) -> str:\n",
        "    if fmt.lower() == \"json\":\n",
        "        out_dict = {\"metadata\": meta, \"results\": []}\n",
        "        for url, content in collector.contents.items():\n",
        "            out_dict[\"results\"].append({\"url\": url, \"content\": content})\n",
        "        return json.dumps(out_dict, indent=2)\n",
        "    elif fmt.lower() == \"md\":\n",
        "        lines = [\"# Crawl4AI Results\", \"\\n## Metadata\", \"```json\", json.dumps(meta, indent=2), \"```\"]\n",
        "        for url, content in collector.contents.items():\n",
        "            lines.append(f\"\\n## {url}\\n```text\\n{content}\\n```\")\n",
        "        return \"\\n\".join(lines)\n",
        "    elif fmt.lower() == \"html\":\n",
        "        lines = [\"<html><head><meta charset='utf-8'><title>Crawl4AI Results</title></head><body>\",\n",
        "                 f\"<h1>Metadata</h1><pre>{json.dumps(meta, indent=2)}</pre>\"]\n",
        "        for url, content in collector.contents.items():\n",
        "            lines.append(f\"<h2>{url}</h2><pre>{content}</pre>\")\n",
        "        lines.append(\"</body></html>\")\n",
        "        return \"\\n\".join(lines)\n",
        "    elif fmt.lower() == \"txt\":\n",
        "        lines = [\"=== Crawl4AI Results ===\", \"METADATA:\", json.dumps(meta, indent=2)]\n",
        "        for url, content in collector.contents.items():\n",
        "            lines.append(f\"\\n--- {url} ---\\n{content}\")\n",
        "        return \"\\n\".join(lines)\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def write_output_files(collector: BFSCollector, meta: dict, base_filename: str, formats: list, max_size_mb: int) -> list:\n",
        "    max_bytes = max_size_mb * 1024 * 1024\n",
        "    output_paths = []\n",
        "    for fmt in formats:\n",
        "        if not fmt:\n",
        "            continue\n",
        "        logger.info(f\"Writing output in {fmt} format...\")\n",
        "        big_str = build_output_string(collector, meta, fmt)\n",
        "        parts = chunk_large_text(big_str, max_bytes)\n",
        "        ext = f\".{fmt.lower()}\"\n",
        "        if len(parts) == 1:\n",
        "            filename = f\"{base_filename}_{fmt.lower()}{ext}\"\n",
        "            filepath = os.path.join(output_dir, filename)\n",
        "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(parts[0])\n",
        "            logger.info(f\"Wrote {filepath} ({len(parts[0].encode('utf-8'))} bytes)\")\n",
        "            output_paths.append(filepath)\n",
        "        else:\n",
        "            for i, chunk in enumerate(parts):\n",
        "                partno = i + 1\n",
        "                filename = f\"{base_filename}_{fmt}_part{partno:03d}{ext}\"\n",
        "                filepath = os.path.join(output_dir, filename)\n",
        "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(chunk)\n",
        "                logger.info(f\"Wrote {filepath} ({len(chunk.encode('utf-8'))} bytes)\")\n",
        "                output_paths.append(filepath)\n",
        "    return output_paths\n",
        "\n",
        "# =============================================================================\n",
        "#       MAIN LOGIC: RUN THE CRAWL\n",
        "# =============================================================================\n",
        "async def run_colab_crawl():\n",
        "    logger.info(\"========== BUILDING CONFIGS ==========\")\n",
        "    # 1) BrowserConfig\n",
        "    bcfg = BrowserConfig(\n",
        "        browser_type=browser_type_choice,\n",
        "        headless=use_headless,\n",
        "        verbose=verbose_browser\n",
        "    )\n",
        "    if enable_proxy and proxy_url.strip():\n",
        "        pconf = {\"server\": proxy_url.strip()}\n",
        "        if proxy_username.strip() and proxy_password.strip():\n",
        "            pconf.update({\"username\": proxy_username.strip(), \"password\": proxy_password.strip()})\n",
        "        bcfg.proxy_config = pconf\n",
        "        logger.info(f\"Using proxy: {pconf}\")\n",
        "    if storage_state_path.strip():\n",
        "        bcfg.storage_state = storage_state_path.strip()\n",
        "        logger.info(f\"Using storage state from: {storage_state_path}\")\n",
        "\n",
        "    # 2) Translate cache mode\n",
        "    cache_map = {\n",
        "        \"ENABLED\": CacheMode.ENABLED,\n",
        "        \"DISABLED\": CacheMode.DISABLED,\n",
        "        \"READ_ONLY\": CacheMode.READ_ONLY,\n",
        "        \"WRITE_ONLY\": CacheMode.WRITE_ONLY,\n",
        "        \"BYPASS\": CacheMode.BYPASS\n",
        "    }\n",
        "    chosen_cache = cache_map.get(cache_mode_choice, CacheMode.BYPASS)\n",
        "\n",
        "    # 3) Build run config\n",
        "    run_config = CrawlerRunConfig(\n",
        "        cache_mode=chosen_cache,\n",
        "        magic=magic_mode,\n",
        "        pdf=want_pdf,\n",
        "        screenshot=want_screenshot,\n",
        "        word_count_threshold=base_word_count_threshold,\n",
        "        scan_full_page=scan_full_page,  # supports JS-heavy pages (e.g., infinite scroll)\n",
        "        scroll_delay=scroll_delay,\n",
        "        wait_for_images=wait_for_images\n",
        "    )\n",
        "\n",
        "    # 4) Set extraction strategy\n",
        "    strat = None\n",
        "    if extraction_strategy_choice == \"NoExtractionStrategy\":\n",
        "        strat = NoExtractionStrategy()\n",
        "        logger.info(\"Using NoExtractionStrategy.\")\n",
        "    elif extraction_strategy_choice == \"JsonCssExtractionStrategy\":\n",
        "        schema = {\"name\": \"JsonCssSample\", \"baseSelector\": jsoncss_base_selector, \"fields\": [{\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"}]}\n",
        "        strat = JsonCssExtractionStrategy(schema=schema)\n",
        "        logger.info(f\"Using JsonCssExtractionStrategy with baseSelector={jsoncss_base_selector}\")\n",
        "    elif extraction_strategy_choice == \"JsonXPathExtractionStrategy\":\n",
        "        schema = {\"name\": \"JsonXPathSample\", \"baseSelector\": jsonxpath_base_selector, \"fields\": [{\"name\": \"title\", \"selector\": \".//h2/text()\", \"type\": \"text\"}]}\n",
        "        from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy\n",
        "        strat = JsonXPathExtractionStrategy(schema=schema)\n",
        "        logger.info(f\"Using JsonXPathExtractionStrategy with baseSelector={jsonxpath_base_selector}\")\n",
        "    elif extraction_strategy_choice == \"LLMExtractionStrategy\":\n",
        "        atoken = llm_api_token.strip() if llm_api_token.strip() else None\n",
        "        strat = LLMExtractionStrategy(provider=llm_provider, api_token=atoken, instruction=llm_instruction_text, input_format=llm_input_format)\n",
        "        logger.info(f\"Using LLMExtractionStrategy: provider={llm_provider}, instruction={llm_instruction_text}\")\n",
        "    elif extraction_strategy_choice == \"CosineStrategy\":\n",
        "        strat = CosineStrategy(semantic_filter=cosine_semantic_filter, word_count_threshold=cosine_word_threshold, sim_threshold=cosine_sim_threshold, top_k=cosine_top_k)\n",
        "        logger.info(f\"Using CosineStrategy: filter={cosine_semantic_filter}, top_k={cosine_top_k}\")\n",
        "    run_config.extraction_strategy = strat\n",
        "\n",
        "    # 5) Concurrency / Dispatcher setup\n",
        "    dispatcher = None\n",
        "    if dispatcher_type != \"None\" and (MemoryAdaptiveDispatcher or SemaphoreDispatcher):\n",
        "        rlim = None\n",
        "        if enable_rate_limit and RateLimiter:\n",
        "            rlim = RateLimiter(base_delay=(rate_base_delay_min, rate_base_delay_max), max_delay=30.0, max_retries=rate_max_retries)\n",
        "        mon = None\n",
        "        if CrawlerMonitor and display_monitor_mode != \"NONE\":\n",
        "            dmode = DisplayMode.DETAILED if display_monitor_mode == \"DETAILED\" else DisplayMode.SIMPLE\n",
        "            mon = CrawlerMonitor(display_mode=dmode)\n",
        "        if dispatcher_type == \"MemoryAdaptiveDispatcher\" and MemoryAdaptiveDispatcher:\n",
        "            dispatcher = MemoryAdaptiveDispatcher(memory_threshold_percent=70.0, max_session_permit=max_session_permit, rate_limiter=rlim, monitor=mon)\n",
        "            logger.info(f\"Using MemoryAdaptiveDispatcher with max_session_permit={max_session_permit}\")\n",
        "        elif dispatcher_type == \"SemaphoreDispatcher\" and SemaphoreDispatcher:\n",
        "            dispatcher = SemaphoreDispatcher(semaphore_count=semaphore_count, rate_limiter=rlim, monitor=mon)\n",
        "            logger.info(f\"Using SemaphoreDispatcher with concurrency={semaphore_count}\")\n",
        "\n",
        "    # 6) Process BFS options and prepare include/exclude lists.\n",
        "    inc_list = [x.strip() for x in include_patterns_csv.split(\",\") if x.strip()]\n",
        "    exc_list = [x.strip() for x in exclude_patterns_csv.split(\",\") if x.strip()]\n",
        "    domain = get_domain(url_to_crawl)\n",
        "\n",
        "    # Prepare config summary for metadata.\n",
        "    config_summary = {\n",
        "        \"start_url\": url_to_crawl,\n",
        "        \"enable_bfs\": enable_bfs,\n",
        "        \"max_depth\": max_depth,\n",
        "        \"max_pages\": max_pages,\n",
        "        \"limit_to_domain\": limit_to_domain,\n",
        "        \"allow_subdomains\": allow_subdomains,\n",
        "        \"disable_reverse_crawling\": disable_reverse_crawling,\n",
        "        \"include_patterns\": inc_list,\n",
        "        \"exclude_patterns\": exc_list,\n",
        "        \"enable_sitemap\": enable_sitemap_parsing,\n",
        "        \"cache_mode\": cache_mode_choice,\n",
        "        \"magic_mode\": magic_mode,\n",
        "        \"pdf\": want_pdf,\n",
        "        \"screenshot\": want_screenshot,\n",
        "        \"browser_type\": browser_type_choice,\n",
        "        \"headless\": use_headless\n",
        "    }\n",
        "\n",
        "    chosen_formats = []\n",
        "    if output_json:\n",
        "        chosen_formats.append(\"json\")\n",
        "    if output_md:\n",
        "        chosen_formats.append(\"md\")\n",
        "    if output_html:\n",
        "        chosen_formats.append(\"html\")\n",
        "    if output_txt:\n",
        "        chosen_formats.append(\"txt\")\n",
        "    if not chosen_formats:\n",
        "        logger.warning(\"No output formats selected! Defaulting to JSON and MD.\")\n",
        "        chosen_formats = [\"json\", \"md\"]\n",
        "\n",
        "    # =============================================================================\n",
        "    #       RUN THE CRAWL\n",
        "    # =============================================================================\n",
        "    try:\n",
        "        async with AsyncWebCrawler(config=bcfg) as crawler:\n",
        "            if enable_hooks:\n",
        "                crawler.crawler_strategy.set_hook(\"before_goto\", custom_before_goto)\n",
        "                crawler.crawler_strategy.set_hook(\"after_goto\", custom_after_goto)\n",
        "            if enable_bfs:\n",
        "                collector = BFSCollector(url_to_crawl, max_pages)\n",
        "                # Enhanced sitemap parsing – inject discovered sitemap URLs into the crawl.\n",
        "                if enable_sitemap_parsing:\n",
        "                    sitemap_links = await enhanced_sitemap_parser(url_to_crawl)\n",
        "                    if sitemap_links:\n",
        "                        logger.info(f\"Sitemap discovered {len(sitemap_links)} links.\")\n",
        "                        for link in sitemap_links:\n",
        "                            if limit_to_domain and not same_domain(link, domain, allow_subdomains):\n",
        "                                continue\n",
        "                            if inc_list and not match_patterns(link, inc_list):\n",
        "                                continue\n",
        "                            if exc_list and match_patterns(link, exc_list):\n",
        "                                continue\n",
        "                            if disable_reverse_crawling and not link.startswith(url_to_crawl):\n",
        "                                continue\n",
        "                            if not collector.is_visited(link):\n",
        "                                collector.mark_visited(link)\n",
        "                                # Inject sitemap URL into BFS queue.\n",
        "                                await do_bfs(link, domain, collector, crawler, run_config, inc_list, exc_list, limit_to_domain, allow_subdomains, max_depth, max_pages, session_id_value.strip() or None, disable_reverse_crawling)\n",
        "                # Run BFS starting from the seed URL.\n",
        "                await do_bfs(\n",
        "                    start_url=collector.start_url,\n",
        "                    domain=domain,\n",
        "                    collector=collector,\n",
        "                    crawler=crawler,\n",
        "                    run_config=run_config,\n",
        "                    includes=inc_list,\n",
        "                    excludes=exc_list,\n",
        "                    limit_to_dom=limit_to_domain,\n",
        "                    allow_subdomains=allow_subdomains,\n",
        "                    max_depth=max_depth,\n",
        "                    max_pages=max_pages,\n",
        "                    session_id=session_id_value.strip() or None,\n",
        "                    disable_reverse_crawling=disable_reverse_crawling\n",
        "                )\n",
        "                meta = collector.get_metadata(config_summary)\n",
        "                output_file_paths = write_output_files(collector, meta, output_base_filename, chosen_formats, max_output_size_mb)\n",
        "            else:\n",
        "                # Single page crawl mode\n",
        "                logger.info(\"Running in single-page crawl mode.\")\n",
        "                collector = BFSCollector(url_to_crawl, 1)\n",
        "                try:\n",
        "                    result = await crawler.arun(url_to_crawl, run_config, session_id=session_id_value.strip() or None)\n",
        "                    if not result.success:\n",
        "                        logger.error(f\"Crawl failed: {result.error_message}\")\n",
        "                        collector.set_result(url_to_crawl, False, result.status_code, error=result.error_message)\n",
        "                    else:\n",
        "                        text = result.markdown or \"\"\n",
        "                        text = re.sub(r\"<https?://[^>]+>\", lambda m: m.group(0).replace(\"<\", \"\").replace(\">\", \"\"), text)\n",
        "                        collector.set_result(url_to_crawl, True, result.status_code, text=text)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Single-page crawl exception: {e}\")\n",
        "                    collector.set_result(url_to_crawl, False, 0, error=str(e))\n",
        "                meta = collector.get_metadata(config_summary)\n",
        "                output_file_paths = write_output_files(collector, meta, output_base_filename, chosen_formats, max_output_size_mb)\n",
        "    except Exception as ex:\n",
        "        logger.error(f\"Top-level crawl exception: {ex}\")\n",
        "        traceback.print_exc()\n",
        "        output_file_paths = []\n",
        "\n",
        "    # New: Automatically download the output files to local machine if running in Colab.\n",
        "    try:\n",
        "        from google.colab import files as gfiles\n",
        "        if auto_download_files and output_file_paths:\n",
        "            logger.info(\"Initiating automatic download of output files...\")\n",
        "            for path in output_file_paths:\n",
        "                logger.info(f\"Downloading {path}...\")\n",
        "                gfiles.download(path)\n",
        "    except ImportError:\n",
        "        logger.info(\"google.colab not available. Skipping automatic file download.\")\n",
        "\n",
        "if execute_crawl:\n",
        "    asyncio.run(run_colab_crawl())\n",
        "else:\n",
        "    logger.info(\"Set 'execute_crawl=True' above and re-run to start crawling.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oN4Mz4fKTIHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}