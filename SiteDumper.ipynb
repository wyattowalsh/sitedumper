{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wyattowalsh/sitedumper/blob/main/SiteDumper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FMJmEAwlm5In",
    "outputId": "682bab7b-91a2-4792-9e89-262c7cacd766"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<main() done, defined at <ipython-input-15-e74b16532cf8>:1387> exception=SystemExit(1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-15-e74b16532cf8>\", line 1404, in main\n",
      "    await dumper.run()\n",
      "          ^^^^^^^^^^\n",
      "AttributeError: 'SiteDumper' object has no attribute 'run'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-e74b16532cf8>\", line 1413, in <cell line: 0>\n",
      "    asyncio.run(main())\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"<ipython-input-15-e74b16532cf8>\", line 1408, in main\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libenchant-2-2 is already the newest version (2.3.2-1ubuntu2).\n",
      "libgraphene-1.0-0 is already the newest version (1.10.8-1).\n",
      "libhyphen0 is already the newest version (2.8.8-7build2).\n",
      "libmanette-0.2-0 is already the newest version (0.2.6-3build1).\n",
      "libsecret-1-0 is already the newest version (0.20.5-2).\n",
      "libwoff1 is already the newest version (1.0.2-1build4).\n",
      "libavif13 is already the newest version (0.9.3-3).\n",
      "libgstreamer-gl1.0-0 is already the newest version (1.20.1-1ubuntu0.4).\n",
      "libgstreamer-plugins-base1.0-0 is already the newest version (1.20.1-1ubuntu0.4).\n",
      "libgtk-4-1 is already the newest version (4.6.9+ds-0ubuntu0.22.04.2).\n",
      "libharfbuzz-icu0 is already the newest version (2.7.4-1ubuntu3.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
      "Camoufox binaries up to date!\n",
      "Current version: v135.0-beta.21\n",
      "Downloading GeoIP database: 100% 58.4M/58.4M [00:00<00:00, 150MiB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-ba99aa5a667e>:137: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator('request_delay_max')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────── SiteDumper Configuration ────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Configuration Summary                                                                           │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Target URL: https://markmap.js.org/docs                                                         │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Max Pages: 1000                                                                                 │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Max Depth: 3                                                                                    │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Output Directory: markmap docs site                                                             │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Export Formats: markdown, json, text, html                                                      │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Browser Profile: random                                                                         │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Stealth Mode: Maximum                                                                           │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────\u001b[0m\u001b[36m SiteDumper Configuration \u001b[0m\u001b[36m───────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mConfiguration Summary\u001b[0m\u001b[36m                                                                         \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mTarget URL: https://markmap.js.org/docs\u001b[0m\u001b[36m                                                       \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mMax Pages: 1000\u001b[0m\u001b[36m                                                                               \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mMax Depth: 3\u001b[0m\u001b[36m                                                                                  \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mOutput Directory: markmap docs site\u001b[0m\u001b[36m                                                           \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mExport Formats: markdown, json, text, html\u001b[0m\u001b[36m                                                    \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mBrowser Profile: random\u001b[0m\u001b[36m                                                                       \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mStealth Mode: Maximum\u001b[0m\u001b[36m                                                                         \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭───────────────────────────────────────── Crawler Status ─────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  SiteDumper v2025.2.7                                                                            │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  User: wyattowalsh                                                                               │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Started at: 2025-02-06 18:05:55 UTC                                                             │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Target: https://markmap.js.org/docs                                                             │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  Output: markmap docs site                                                                       │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m────────────────────────────────────────\u001b[0m\u001b[36m Crawler Status \u001b[0m\u001b[36m────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mSiteDumper v2025.2.7\u001b[0m\u001b[36m                                                                          \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mUser: wyattowalsh\u001b[0m\u001b[36m                                                                             \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mStarted at: 2025-02-06 18:05:55 UTC\u001b[0m\u001b[36m                                                           \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mTarget: https://markmap.js.org/docs\u001b[0m\u001b[36m                                                           \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36mOutput: markmap docs site\u001b[0m\u001b[36m                                                                     \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-5' coro=<Connection.run() done, defined at /usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py:272> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/_impl/_connection.py\", line 279, in run\n",
      "    await self._transport.connect()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/_impl/_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/_impl/_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/unix_events.py\", line 198, in _make_subprocess_transport\n",
      "    with events.get_child_watcher() as watcher:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 815, in get_child_watcher\n",
      "    return get_event_loop_policy().get_child_watcher()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 641, in get_child_watcher\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">╭──────────────────────────────────────── </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crawl Complete!</span><span style=\"color: #008080; text-decoration-color: #008080\"> ─────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  </span><span style=\"color: #008080; text-decoration-color: #008080; font-style: italic\">      Crawler Summary Report      </span><span style=\"color: #008080; text-decoration-color: #008080\">                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  ┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  ┃</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Metric        </span><span style=\"color: #008080; text-decoration-color: #008080\">┃</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">          Value </span><span style=\"color: #008080; text-decoration-color: #008080\">┃                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  ┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Duration      │</span><span style=\"color: #008000; text-decoration-color: #008000\"> 0:00:00.193979 </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Total Pages   │</span><span style=\"color: #008000; text-decoration-color: #008000\">              0 </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Total Data    │</span><span style=\"color: #008000; text-decoration-color: #008000\">        0.00 MB </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Average Speed │</span><span style=\"color: #008000; text-decoration-color: #008000\">   0.00 pages/s </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Success Rate  │</span><span style=\"color: #008000; text-decoration-color: #008000\">           0.0% </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Peak Memory   │</span><span style=\"color: #008000; text-decoration-color: #008000\">         0.0 MB </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Unique URLs   │</span><span style=\"color: #008000; text-decoration-color: #008000\">              0 </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Error Count   │</span><span style=\"color: #008000; text-decoration-color: #008000\">              0 </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Status Codes  │</span><span style=\"color: #008000; text-decoration-color: #008000\">                </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  │ Content Types │</span><span style=\"color: #008000; text-decoration-color: #008000\">                </span><span style=\"color: #008080; text-decoration-color: #008080\">│                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│  └───────────────┴────────────────┘                                                              │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">│                                                                                                  │</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36m╭─\u001b[0m\u001b[36m───────────────────────────────────────\u001b[0m\u001b[36m \u001b[0m\u001b[1;36mCrawl Complete!\u001b[0m\u001b[36m \u001b[0m\u001b[36m────────────────────────────────────────\u001b[0m\u001b[36m─╮\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[3;36m      Crawler Summary Report      \u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m┃\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMetric       \u001b[0m\u001b[1;35m \u001b[0m\u001b[36m┃\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m         Value\u001b[0m\u001b[1;35m \u001b[0m\u001b[36m┃\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mDuration     \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m0:00:00.193979\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mTotal Pages  \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m             0\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mTotal Data   \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m       0.00 MB\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mAverage Speed\u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m  0.00 pages/s\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mSuccess Rate \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m          0.0%\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mPeak Memory  \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m        0.0 MB\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mUnique URLs  \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m             0\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mError Count  \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m             0\u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mStatus Codes \u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m              \u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\u001b[36m \u001b[0m\u001b[36mContent Types\u001b[0m\u001b[36m \u001b[0m\u001b[36m│\u001b[0m\u001b[32m \u001b[0m\u001b[32m              \u001b[0m\u001b[32m \u001b[0m\u001b[36m│\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m  \u001b[0m\u001b[36m└───────────────┴────────────────┘\u001b[0m\u001b[36m                                                            \u001b[0m\u001b[36m  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m│\u001b[0m\u001b[36m                                                                                                  \u001b[0m\u001b[36m│\u001b[0m\n",
       "\u001b[36m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-ba99aa5a667e>:1209: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  \"configuration\": self.config.dict(),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">Fatal Error:</span><span style=\"color: #008080; text-decoration-color: #008080\"> Object of type HttpUrl is not JSON serializable</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mFatal Error:\u001b[0m\u001b[36m Object of type HttpUrl is not JSON serializable\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1121, in run\n",
      "    await self.initialize_browser()\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 878, in initialize_browser\n",
      "    playwright = await async_playwright().start()\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/async_api/_context_manager.py\", line 51, in start\n",
      "    return await self.__aenter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/async_api/_context_manager.py\", line 46, in __aenter__\n",
      "    playwright = AsyncPlaywright(next(iter(done)).result())\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/playwright/_impl/_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/subprocess.py\", line 223, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1708, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/unix_events.py\", line 198, in _make_subprocess_transport\n",
      "    with events.get_child_watcher() as watcher:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 815, in get_child_watcher\n",
      "    return get_event_loop_policy().get_child_watcher()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 641, in get_child_watcher\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1404, in main\n",
      "    await dumper.run()\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1177, in run\n",
      "    await self.generate_final_report()\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1225, in generate_final_report\n",
      "    await f.write(json.dumps(report, indent=2))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "          ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 202, in encode\n",
      "    chunks = list(chunks)\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type HttpUrl is not JSON serializable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1413, in <cell line: 0>\n",
      "    asyncio.run(main())\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"<ipython-input-17-ba99aa5a667e>\", line 1408, in main\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
      "    traceback_info = getframeinfo(tb, context)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
      "    lineno = frame.f_lineno\n",
      "             ^^^^^^^^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;31m# Initialize browser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_browser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36minitialize_browser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;34m\"\"\"Initialize and configure browser instance.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0mplaywright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masync_playwright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/async_api/_context_manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncPlaywright\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/async_api/_context_manager.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mplaywright_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mplaywright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAsyncPlaywright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mplaywright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aexit__\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/playwright/_impl/_transport.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mexecutable_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrypoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_driver_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             self._proc = await asyncio.create_subprocess_exec(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mexecutable_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/subprocess.py\u001b[0m in \u001b[0;36mcreate_subprocess_exec\u001b[0;34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                                         loop=loop)\n\u001b[0;32m--> 223\u001b[0;31m     transport, protocol = await loop.subprocess_exec(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mprotocol_factory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36msubprocess_exec\u001b[0;34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_subprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         transport = await self._make_subprocess_transport(\n\u001b[0m\u001b[1;32m   1709\u001b[0m             \u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopen_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/unix_events.py\u001b[0m in \u001b[0;36m_make_subprocess_transport\u001b[0;34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                          extra=None, **kwargs):\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child_watcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/events.py\u001b[0m in \u001b[0;36mget_child_watcher\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;34m\"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_event_loop_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child_watcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/events.py\u001b[0m in \u001b[0;36mget_child_watcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;34m\"Get the watcher for child processes.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mdumper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_final_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36mgenerate_final_report\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maiofiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type HttpUrl is not JSON serializable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0muvloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# that return non-generator iterators from their `__iter__`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Needed to break cycles when an exception occurs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ba99aa5a667e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0mconsole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[red]Fatal Error:[/red] {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SiteDumper Notebook - Advanced Intelligent Website Content Harvester\n",
    "Version: 2025.2.7\n",
    "Last Updated: 2025-02-07 12:00:00 UTC\n",
    "Author: wyattowalsh\n",
    "\"\"\"\n",
    "from datetime import datetime, timezone\n",
    "#@title 📅 Current Time and User {display-mode: \"form\"}\n",
    "USER_LOGIN = \"w4w\" #@param {type:\"string\"}\n",
    "CURRENT_UTC = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\") # Automatically set current UTC time\n",
    "\n",
    "\n",
    "!apt-get update && apt-get install -y \\\n",
    "        libgtk-4-1 libgraphene-1.0-0 libwoff1 \\\n",
    "        libgstreamer-gl1.0-0 libgstreamer-plugins-base1.0-0 \\\n",
    "        libavif13 libharfbuzz-icu0 libenchant-2-2 \\\n",
    "        libsecret-1-0 libhyphen0 libmanette-0.2-0\n",
    "\n",
    "%pip install -q \\\n",
    "    playwright \\\n",
    "    camoufox[geoip] \\\n",
    "    docling \\\n",
    "    docling-core \\\n",
    "    pydantic[email] \\\n",
    "    rich \\\n",
    "    loguru \\\n",
    "    nest-asyncio \\\n",
    "    uvloop \\\n",
    "    tqdm \\\n",
    "    beautifulsoup4 \\\n",
    "    lxml \\\n",
    "    python-docx \\\n",
    "    xmltodict \\\n",
    "    requests \\\n",
    "    pandas \\\n",
    "    aiohttp \\\n",
    "    asyncio-throttle \\\n",
    "    pyppeteer \\\n",
    "    aiofiles \\\n",
    "    python-magic \\\n",
    "    fastapi \\\n",
    "    uvicorn \\\n",
    "    colorama \\\n",
    "    yaspin \\\n",
    "    halo \\\n",
    "    psutil \\\n",
    "    xmltodict\n",
    "\n",
    "!playwright install chromium\n",
    "!python3 -m camoufox fetch\n",
    "\n",
    "#@title 📚 Import Required Libraries {display-mode: \"code\"}\n",
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "import glob\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "import gzip\n",
    "import random\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Dict, List, Optional, Set, Union, Any, Generator, Literal\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from functools import partial, wraps\n",
    "from collections import deque, Counter, defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import xmltodict\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import pandas as pd\n",
    "import aiohttp\n",
    "from io import BytesIO, StringIO\n",
    "import hashlib\n",
    "import time\n",
    "import signal\n",
    "import statistics\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import aiofiles\n",
    "import magic\n",
    "from fastapi import FastAPI, HTTPException, Query, BackgroundTasks\n",
    "import uvicorn\n",
    "from asyncio_throttle import Throttler\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
    "from camoufox import AsyncCamoufox, AsyncNewBrowser\n",
    "from pydantic import BaseModel, EmailStr, HttpUrl, validator, Field\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn, MofNCompleteColumn\n",
    "from rich.panel import Panel\n",
    "from rich.style import Style\n",
    "from rich.text import Text\n",
    "from rich.layout import Layout\n",
    "from rich.live import Live\n",
    "from rich.table import Table\n",
    "from rich.console import Group\n",
    "from rich.align import Align\n",
    "from loguru import logger\n",
    "import nest_asyncio\n",
    "import uvloop\n",
    "from yaspin import yaspin\n",
    "from halo import Halo\n",
    "from colorama import init, Fore, Back, Style as ColoramaStyle\n",
    "from docling_core.types.doc import DoclingDocument, ProvenanceItem, DocItemLabel, BoundingBox, CoordOrigin\n",
    "\n",
    "\n",
    "# Initialize colorama for cross-platform colored output\n",
    "init(autoreset=True)\n",
    "\n",
    "#@title 🔧 Configuration Class Definition {display-mode: \"code\"}\n",
    "\n",
    "# Configuration class using Pydantic\n",
    "class Config(BaseModel):\n",
    "    \"\"\"Enhanced configuration with validation and descriptions.\"\"\"\n",
    "    # Basic settings\n",
    "    user_login: str = Field(..., description=\"User login name.\")\n",
    "    current_utc: str = Field(..., description=\"Current UTC time.\")\n",
    "    output_dir: str = Field(\"sitedumper_output\", description=\"Base directory for all outputs.\")\n",
    "\n",
    "    # Website settings\n",
    "    website_url: HttpUrl = Field(..., description=\"URL of the website to crawl.\")\n",
    "    max_pages: int = Field(1000, ge=1, description=\"Maximum number of pages to crawl.\")\n",
    "    max_depth: int = Field(3, ge=0, description=\"Maximum crawl depth.\")\n",
    "    stay_on_domain: bool = Field(True, description=\"Restrict crawling to the initial domain.\")\n",
    "\n",
    "    # Compliance settings\n",
    "    obey_robots_txt: bool = Field(True, description=\"Follow rules in robots.txt.\")\n",
    "    process_sitemaps: bool = Field(True, description=\"Process sitemaps to discover URLs.\")\n",
    "    recursive_sitemap: bool = Field(True, description=\"Recursively process nested sitemaps.\")\n",
    "    sitemap_only: bool = Field(False, description=\"Only crawl URLs found in sitemaps.\")\n",
    "\n",
    "    # Path patterns\n",
    "    include_patterns: List[str] = Field(default=[], description=\"URL patterns to include.\")\n",
    "    exclude_patterns: List[str] = Field(default=[], description=\"URL patterns to exclude.\")\n",
    "\n",
    "    # Browser settings\n",
    "    stealth_mode: Literal[\"Basic\", \"Moderate\", \"Maximum\"] = Field(\"Maximum\", description=\"Level of browser stealth.\")\n",
    "    browser_profile: Literal[\"random\", \"mobile\", \"desktop\"] = Field(\"random\", description=\"Browser profile type.\")\n",
    "    use_proxy: bool = Field(False, description=\"Use a proxy server for requests.\")\n",
    "    proxy_url: Optional[str] = Field(None, description=\"Proxy server URL (if use_proxy is True).\")\n",
    "\n",
    "    # Performance settings\n",
    "    request_delay_min: float = Field(1.0, ge=0.0, description=\"Minimum delay between requests.\")\n",
    "    request_delay_max: float = Field(3.0, ge=0.0, description=\"Maximum delay between requests.\")\n",
    "    max_concurrent: int = Field(5, ge=1, description=\"Maximum concurrent requests.\")\n",
    "    request_timeout: int = Field(30, ge=1, description=\"Request timeout in seconds.\")\n",
    "    retry_attempts: int = Field(3, ge=1, description=\"Number of retry attempts for failed requests.\")\n",
    "\n",
    "    # Output settings\n",
    "    export_formats: List[str] = Field(default=[\"markdown\", \"json\", \"text\", \"html\"], description=\"Formats to export the scraped content.\")\n",
    "    compress_output: bool = Field(True, description=\"Compress output files using gzip.\")\n",
    "    structure_output: bool = Field(True, description=\"Organize output files in directories mirroring site structure.\")\n",
    "    maintain_hierarchy: bool = Field(True, description=\"Preserve URL hierarchy in output file paths.\")\n",
    "    clean_output: bool = Field(True, description=\"Remove unnecessary elements from HTML.\")\n",
    "\n",
    "    # Export options\n",
    "    export_metadata: bool = Field(True, description=\"Include metadata in output files.\")\n",
    "    export_stats: bool = Field(True, description=\"Export crawler statistics.\")\n",
    "    export_sitemap: bool = Field(True, description=\"Export discovered sitemap URLs.\")\n",
    "    include_timestamps: bool = Field(True, description=\"Include timestamps in output data.\")\n",
    "    include_checksums: bool = Field(True, description=\"Include checksums for content verification.\")\n",
    "    generate_report: bool = Field(True, description=\"Generate a comprehensive crawl report.\")\n",
    "\n",
    "    # API settings\n",
    "    enable_api: bool = Field(False, description=\"Enable the REST API for monitoring and control.\")\n",
    "    api_port: int = Field(8000, ge=1024, le=65535, description=\"Port for the API server.\")\n",
    "\n",
    "    @validator('request_delay_max')\n",
    "    def validate_delay_max(cls, v, values):\n",
    "        \"\"\"Ensure max delay is not less than min delay.\"\"\"\n",
    "        if 'request_delay_min' in values and v < values['request_delay_min']:\n",
    "            raise ValueError('request_delay_max must be greater than or equal to request_delay_min')\n",
    "        return v\n",
    "\n",
    "class CrawlerStats:\n",
    "    \"\"\"Enhanced statistics tracking with real-time visualization using rich.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.start_time = datetime.now(timezone.utc)\n",
    "        self.pages_processed = 0\n",
    "        self.bytes_downloaded = 0\n",
    "        self.download_times = []\n",
    "        self.processing_times = []\n",
    "        self.errors = Counter()\n",
    "        self.status_codes = Counter()\n",
    "        self.content_types = Counter()\n",
    "        self.urls_processed = set()\n",
    "        self.skipped_urls = {}\n",
    "        self.current_memory_usage = 0\n",
    "        self.peak_memory_usage = 0\n",
    "        self.last_update = time.time()\n",
    "        self.update_interval = 1.0  # Update stats every second\n",
    "\n",
    "        # Real-time visualization setup\n",
    "        self.progress_table = Table(\n",
    "            title=\"Crawler Statistics\",\n",
    "            show_header=True,\n",
    "            header_style=\"bold magenta\",\n",
    "            border_style=\"cyan\",\n",
    "            box=None\n",
    "        )\n",
    "        self.progress_table.add_column(\"Metric\", style=\"cyan\")\n",
    "        self.progress_table.add_column(\"Value\", justify=\"right\", style=\"green\")\n",
    "\n",
    "    def update_memory_usage(self):\n",
    "        \"\"\"Update memory usage statistics.\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        self.current_memory_usage = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        self.peak_memory_usage = max(self.peak_memory_usage, self.current_memory_usage)\n",
    "\n",
    "    def get_progress_display(self) -> Panel:\n",
    "        \"\"\"Generate rich progress display.\"\"\"\n",
    "        self.update_memory_usage()\n",
    "\n",
    "        # Update progress table\n",
    "        self.progress_table.clear()\n",
    "        stats = [\n",
    "            (\"Pages Processed\", f\"{self.pages_processed:,}\"),\n",
    "            (\"Data Downloaded\", f\"{self.bytes_downloaded / 1024 / 1024:.2f} MB\"),\n",
    "            (\"Average Speed\", f\"{self.get_average_speed():.2f} pages/s\"),\n",
    "            (\"Current Memory\", f\"{self.current_memory_usage:.1f} MB\"),\n",
    "            (\"Peak Memory\", f\"{self.peak_memory_usage:.1f} MB\"),\n",
    "            (\"Success Rate\", f\"{self.get_success_rate():.1f}%\"),\n",
    "            (\"Active URLs\", f\"{len(self.urls_processed):,}\"),\n",
    "            (\"Error Count\", f\"{sum(self.errors.values()):,}\")\n",
    "        ]\n",
    "\n",
    "        for metric, value in stats:\n",
    "            self.progress_table.add_row(metric, value)\n",
    "\n",
    "        return Panel(\n",
    "            Align.center(self.progress_table),\n",
    "            title=\"[bold cyan]SiteDumper Status[/bold cyan]\",\n",
    "            subtitle=f\"[dim]Running since: {self.start_time.strftime('%Y-%m-%d %H:%M:%S UTC')}[/dim]\",\n",
    "            border_style=\"cyan\",\n",
    "            padding=(1, 2)\n",
    "        )\n",
    "    def get_average_speed(self) -> float:\n",
    "        \"\"\"Calculate average processing speed.\"\"\"\n",
    "        duration = (datetime.now(timezone.utc) - self.start_time).total_seconds()\n",
    "        return self.pages_processed / duration if duration > 0 and self.pages_processed > 0 else 0.0\n",
    "\n",
    "    def get_success_rate(self) -> float:\n",
    "        \"\"\"Calculate success rate.\"\"\"\n",
    "        total = self.pages_processed + sum(self.errors.values())\n",
    "        return (self.pages_processed / total * 100) if total > 0 else 0.0\n",
    "\n",
    "    def add_page(self, url: str, size: int, content_type: str, status_code: int):\n",
    "        \"\"\"Record processed page statistics.\"\"\"\n",
    "        self.pages_processed += 1\n",
    "        self.bytes_downloaded += size\n",
    "        self.status_codes[status_code] += 1\n",
    "        self.content_types[content_type] += 1\n",
    "        self.urls_processed.add(url)\n",
    "\n",
    "        # Update display if needed\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_update >= self.update_interval:\n",
    "            self.last_update = current_time\n",
    "            console.print(self.get_progress_display())\n",
    "\n",
    "\n",
    "    def add_error(self, url: str, error: str):\n",
    "        \"\"\"Record error statistics.\"\"\"\n",
    "        self.errors[error] += 1\n",
    "        self.skipped_urls[url] = error\n",
    "        console.print(f\"[red]Error processing {url}: {error}[/red]\")\n",
    "\n",
    "    def add_skipped(self, url: str, reason: str):\n",
    "        \"\"\"Record skipped URL statistics.\"\"\"\n",
    "        self.skipped_urls[url] = reason\n",
    "        console.print(f\"[yellow]Skipped {url}: {reason}[/yellow]\")\n",
    "\n",
    "    def add_download_time(self, time: float):\n",
    "        \"\"\"Record download time.\"\"\"\n",
    "        self.download_times.append(time)\n",
    "\n",
    "    def add_processing_time(self, time: float):\n",
    "        \"\"\"Record processing time.\"\"\"\n",
    "        self.processing_times.append(time)\n",
    "\n",
    "    def finish(self):\n",
    "        \"\"\"Complete statistics tracking and generate final report.\"\"\"\n",
    "        self.end_time = datetime.now(timezone.utc)\n",
    "        self.generate_final_report()\n",
    "\n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"Get current statistics summary.\"\"\"\n",
    "        return {\n",
    "            \"pages_processed\": self.pages_processed,\n",
    "            \"bytes_downloaded\": self.bytes_downloaded,\n",
    "            \"average_speed\": self.get_average_speed(),\n",
    "            \"success_rate\": self.get_success_rate(),\n",
    "            \"errors\": dict(self.errors),\n",
    "            \"status_codes\": dict(self.status_codes),\n",
    "            \"content_types\": dict(self.content_types),\n",
    "            \"memory_usage\": {\n",
    "                \"current\": self.current_memory_usage,\n",
    "                \"peak\": self.peak_memory_usage\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate and display the final crawler report.\"\"\"\n",
    "        duration = self.end_time - self.start_time\n",
    "\n",
    "        summary_table = Table(\n",
    "            title=\"Crawler Summary Report\",\n",
    "            show_header=True,\n",
    "            header_style=\"bold magenta\",\n",
    "            border_style=\"cyan\"\n",
    "        )\n",
    "\n",
    "        summary_table.add_column(\"Metric\", style=\"cyan\")\n",
    "        summary_table.add_column(\"Value\", justify=\"right\", style=\"green\")\n",
    "\n",
    "        summary_stats = [\n",
    "            (\"Duration\", str(duration)),\n",
    "            (\"Total Pages\", f\"{self.pages_processed:,}\"),\n",
    "            (\"Total Data\", f\"{self.bytes_downloaded / 1024 / 1024:.2f} MB\"),\n",
    "            (\"Average Speed\", f\"{self.get_average_speed():.2f} pages/s\"),\n",
    "            (\"Success Rate\", f\"{self.get_success_rate():.1f}%\"),\n",
    "            (\"Peak Memory\", f\"{self.peak_memory_usage:.1f} MB\"),\n",
    "            (\"Unique URLs\", f\"{len(self.urls_processed):,}\"),\n",
    "            (\"Error Count\", f\"{sum(self.errors.values()):,}\"),\n",
    "            (\"Status Codes\", \", \".join(f\"{k}: {v}\" for k, v in self.status_codes.most_common())),\n",
    "            (\"Content Types\", \", \".join(f\"{k}: {v}\" for k, v in self.content_types.most_common(3)))\n",
    "        ]\n",
    "\n",
    "        for metric, value in summary_stats:\n",
    "            summary_table.add_row(metric, value)\n",
    "\n",
    "        console.print(\"\\n\")\n",
    "        console.print(Panel(\n",
    "            summary_table,\n",
    "            title=\"[bold cyan]Crawl Complete![/bold cyan]\",\n",
    "            border_style=\"cyan\",\n",
    "            padding=(1, 2)\n",
    "        ))\n",
    "\n",
    "class URLQueue:\n",
    "    \"\"\"Manages URLs to be crawled, prioritizing by depth.\"\"\"\n",
    "    def __init__(self, max_depth: int):\n",
    "        self.queue = deque()\n",
    "        self.processed_urls = set()\n",
    "        self.max_depth = max_depth\n",
    "        self.lock = asyncio.Lock()  # Ensure thread safety\n",
    "\n",
    "    async def add_url(self, url: str, depth: int = 0, referrer: Optional[str] = None):\n",
    "        \"\"\"Add a URL to the queue if it hasn't been processed and is within max depth.\"\"\"\n",
    "        async with self.lock:\n",
    "            if url not in self.processed_urls and depth <= self.max_depth:\n",
    "                self.queue.append({\"url\": url, \"depth\": depth, \"referrer\": referrer})\n",
    "                self.processed_urls.add(url)\n",
    "\n",
    "    async def get_next(self) -> Optional[dict]:\n",
    "        \"\"\"Retrieve the next URL from the queue, prioritizing by depth.\"\"\"\n",
    "        async with self.lock:\n",
    "            if self.queue:\n",
    "                return self.queue.popleft()  # FIFO for breadth-first\n",
    "            return None\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Return statistics about the queue.\"\"\"\n",
    "        return {\n",
    "            \"queued\": len(self.queue),\n",
    "            \"processed\": len(self.processed_urls),\n",
    "            \"max_depth\": self.max_depth\n",
    "        }\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"Check if the queue is empty.\"\"\"\n",
    "        return len(self.queue) == 0\n",
    "\n",
    "class ContentProcessor:\n",
    "    \"\"\"Advanced content processor with docling integration.\"\"\"\n",
    "    def __init__(self, clean_output: bool = True):\n",
    "        self.clean_output = clean_output\n",
    "        self.mime_detector = magic.Magic(mime=True)\n",
    "        self.content_stats = Counter()\n",
    "\n",
    "    async def process_content(self, url: str, content: str, response_headers: dict) -> dict:\n",
    "        \"\"\"Process content with advanced features and error handling.\"\"\"\n",
    "        try:\n",
    "            # Detect content type\n",
    "            content_type = response_headers.get('content-type',\n",
    "                self.mime_detector.from_buffer(content.encode()))\n",
    "            self.content_stats[content_type] += 1\n",
    "\n",
    "            # Create docling document\n",
    "            doc = DoclingDocument()\n",
    "\n",
    "            # Create soup object with error handling\n",
    "            try:\n",
    "                soup = BeautifulSoup(content, 'lxml')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to parse HTML with lxml, falling back to html.parser: {e}\")\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Extract and process content\n",
    "            title = self._extract_title(soup, url)\n",
    "            meta_tags = self._extract_meta_tags(soup)\n",
    "            links = self._extract_links(soup, url)\n",
    "            images = self._extract_images(soup, url)\n",
    "\n",
    "            # Process main content\n",
    "            text = self._extract_clean_text(soup)\n",
    "\n",
    "            # Create text item with provenance\n",
    "            text_item = doc.add_text(\n",
    "                text=text,\n",
    "                prov=ProvenanceItem(\n",
    "                    page_no=1,  # Single page for web content\n",
    "                    charspan=(0, len(text)),\n",
    "                    bbox=BoundingBox(  # Placeholder bbox\n",
    "                        x1=0, y1=0, x2=100, y2=100,\n",
    "                        origin=CoordOrigin.TOPLEFT\n",
    "                    )\n",
    "                ),\n",
    "                label=DocItemLabel.PARAGRAPH\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'meta_tags': meta_tags,\n",
    "                'text': text,\n",
    "                'links': links,\n",
    "                'images': images,\n",
    "                'content_type': content_type,\n",
    "                'doc_model': doc,  # Include docling document model\n",
    "                'checksums': self._generate_checksums(content),\n",
    "                'stats': {\n",
    "                    'text_length': len(text),\n",
    "                    'link_count': len(links),\n",
    "                    'image_count': len(images),\n",
    "                    'processing_timestamp': datetime.now(timezone.utc).isoformat()\n",
    "                }\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing content for {url}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_clean_text(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract and clean text content with contextual awareness.\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        for unwanted in ['script', 'style', 'noscript', 'iframe', 'head']:\n",
    "            for elem in soup.find_all(unwanted):\n",
    "                elem.decompose()\n",
    "\n",
    "        # Extract text with structure preservation\n",
    "        paragraphs = []  # Initialize as an empty list to store paragraphs\n",
    "\n",
    "        for elem in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
    "            text = elem.get_text(strip=True)\n",
    "            if text:\n",
    "                if elem.name.startswith('h'):\n",
    "                    # Preserve heading structure\n",
    "                    level = int(elem.name[1])\n",
    "                    paragraphs.append(f\"{'#' * level} {text}\")\n",
    "                elif elem.name == 'li':\n",
    "                    # Preserve list items\n",
    "                    paragraphs.append(f\"- {text}\")\n",
    "                else:\n",
    "                    paragraphs.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "    def _extract_title(self, soup: BeautifulSoup, url: str) -> str:\n",
    "        \"\"\"Extract page title with enhanced fallbacks.\"\"\"\n",
    "        title = None\n",
    "\n",
    "        # Try different title sources in priority order\n",
    "        if soup.title:\n",
    "            title = soup.title.string\n",
    "        if not title and soup.find('meta', property='og:title'):\n",
    "            title = soup.find('meta', property='og:title')['content']\n",
    "        if not title and soup.find('h1'):\n",
    "            title = soup.find('h1').get_text(strip=True)\n",
    "        if not title and soup.find('meta', {'name': 'twitter:title'}):\n",
    "            title = soup.find('meta', {'name': 'twitter:title'})['content']\n",
    "\n",
    "        # Clean and normalize title\n",
    "        if title:\n",
    "            title = ' '.join(title.split())\n",
    "\n",
    "        return title or urlparse(url).path.split('/')[-1] or url\n",
    "\n",
    "    def _extract_meta_tags(self, soup: BeautifulSoup) -> dict:\n",
    "        \"\"\"Extract comprehensive metadata.\"\"\"\n",
    "        meta_tags = {}\n",
    "\n",
    "        # Standard meta tags\n",
    "        for meta in soup.find_all('meta'):\n",
    "            name = meta.get('name', meta.get('property', ''))\n",
    "            content = meta.get('content', '')\n",
    "            if name and content:\n",
    "                meta_tags[name] = content\n",
    "\n",
    "        # OpenGraph tags\n",
    "        for meta in soup.find_all('meta', property=re.compile('^og:')):\n",
    "            meta_tags[meta['property']] = meta.get('content', '')\n",
    "\n",
    "        # Twitter cards\n",
    "        for meta in soup.find_all('meta', name=re.compile('^twitter:')):\n",
    "            meta_tags[meta['name']] = meta.get('content', '')\n",
    "\n",
    "        # Schema.org metadata\n",
    "        for script in soup.find_all('script', type='application/ld+json'):\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "                if isinstance(data, dict):\n",
    "                    meta_tags['schema_org'] = data\n",
    "            except (json.JSONDecodeError, AttributeError):\n",
    "                continue\n",
    "\n",
    "        return meta_tags\n",
    "\n",
    "    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[dict]]:\n",
    "        \"\"\"Extract and categorize links with enhanced metadata.\"\"\"\n",
    "        links = defaultdict(list)\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = urljoin(base_url, a['href'])\n",
    "            text = a.get_text(strip=True)\n",
    "            title = a.get('title', '')\n",
    "            rel = a.get('rel',)\n",
    "\n",
    "            link_data = {\n",
    "                'url': href,\n",
    "                'text': text,\n",
    "                'title': title,\n",
    "                'rel': rel,\n",
    "                'class': a.get('class',),\n",
    "                'id': a.get('id', ''),\n",
    "                'aria_label': a.get('aria-label', '')\n",
    "            }\n",
    "\n",
    "            # Categorize link\n",
    "            if href.startswith('mailto:'):\n",
    "                links['email'].append(link_data)\n",
    "            elif href.startswith('tel:'):\n",
    "                links['phone'].append(link_data)\n",
    "            elif urlparse(href).netloc == urlparse(base_url).netloc:\n",
    "                links['internal'].append(link_data)\n",
    "            else:\n",
    "                links['external'].append(link_data)\n",
    "\n",
    "        return dict(links)\n",
    "\n",
    "    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[dict]:\n",
    "        \"\"\"Extract comprehensive image information.\"\"\"\n",
    "        images = []  # Initialize images list to avoid syntax errors\n",
    "\n",
    "        for img in soup.find_all('img'):\n",
    "            src = img.get('src', '')\n",
    "            if src:\n",
    "                image_url = urljoin(base_url, src)\n",
    "\n",
    "                # Extract all available image metadata\n",
    "                image_data = {\n",
    "                    'url': image_url,\n",
    "                    'alt': img.get('alt', ''),\n",
    "                    'title': img.get('title', ''),\n",
    "                    'width': img.get('width', ''),\n",
    "                    'height': img.get('height', ''),\n",
    "                    'class': img.get('class', []),\n",
    "                    'id': img.get('id', ''),\n",
    "                    'loading': img.get('loading', ''),\n",
    "                    'srcset': img.get('srcset', ''),\n",
    "                    'sizes': img.get('sizes', ''),\n",
    "                    'figure_caption': None\n",
    "                }\n",
    "\n",
    "                # Look for associated figure caption\n",
    "                figure_parent = img.find_parent('figure')\n",
    "                if figure_parent:\n",
    "                    figcaption = figure_parent.find('figcaption')\n",
    "                    if figcaption:\n",
    "                        image_data['figure_caption'] = figcaption.get_text(strip=True)\n",
    "\n",
    "                images.append(image_data)\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "    def _generate_checksums(self, content: str) -> dict:\n",
    "        \"\"\"Generate multiple checksums for content verification.\"\"\"\n",
    "        content_bytes = content.encode('utf-8')\n",
    "        return {\n",
    "            'md5': hashlib.md5(content_bytes).hexdigest(),\n",
    "            'sha1': hashlib.sha1(content_bytes).hexdigest(),\n",
    "            'sha256': hashlib.sha256(content_bytes).hexdigest()\n",
    "        }\n",
    "\n",
    "class PathFilter:\n",
    "    \"\"\"Enhanced path filtering with pattern matching.\"\"\"\n",
    "    def __init__(self, include_patterns: List[str], exclude_patterns: List[str]):\n",
    "        self.include_patterns = [p.strip() for p in include_patterns if p.strip()]\n",
    "        self.exclude_patterns = [p.strip() for p in exclude_patterns if p.strip()]\n",
    "\n",
    "    def should_process(self, url: str) -> bool:\n",
    "        \"\"\"Determine if URL should be processed based on patterns.\"\"\"\n",
    "        path = urlparse(url).path\n",
    "\n",
    "        # Check exclude patterns first\n",
    "        for pattern in self.exclude_patterns:\n",
    "            if fnmatch.fnmatch(path, pattern):\n",
    "                return False\n",
    "\n",
    "        # If include patterns exist, path must match at least one\n",
    "        if self.include_patterns:\n",
    "            return any(fnmatch.fnmatch(path, pattern) for pattern in self.include_patterns)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "class RobotsProcessor:\n",
    "    \"\"\"Enhanced robots.txt processor with caching and retry logic.\"\"\"\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "        self.robots_url = urljoin(base_url, \"/robots.txt\")\n",
    "        self.parser = RobotFileParser(self.robots_url)\n",
    "        self.cache = {}\n",
    "        self.cache_time = 3600  # Cache for 1 hour\n",
    "        self.last_fetch = 0\n",
    "        self.max_retries = 3\n",
    "        self.initialized = False\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize robots.txt parser with retry logic.\"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    async with session.get(self.robots_url) as response:\n",
    "                        if response.status == 200:\n",
    "                            content = await response.text()\n",
    "                            self.parser.parse(content.splitlines())\n",
    "                            self.last_fetch = time.time()\n",
    "                            self.initialized = True\n",
    "                            break\n",
    "                        elif response.status == 404:\n",
    "                            # No robots.txt, allow everything\n",
    "                            self.initialized = True\n",
    "                            break\n",
    "                except Exception as e:\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        logger.warning(f\"Failed to fetch robots.txt: {e}\")\n",
    "                        self.initialized = True  # Proceed without robots.txt\n",
    "                    await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "    def can_fetch(self, url: str) -> bool:\n",
    "        \"\"\"Check if URL can be fetched according to robots.txt.\"\"\"\n",
    "        if not self.initialized:\n",
    "            return True  # Allow if not initialized\n",
    "\n",
    "        # Check cache first\n",
    "        cache_key = url\n",
    "        cached_result = self.cache.get(cache_key)\n",
    "        if cached_result and time.time() - cached_result['time'] < self.cache_time:\n",
    "            return cached_result['allowed']\n",
    "\n",
    "        # Check robots.txt\n",
    "        allowed = True  # Default to allowed\n",
    "        if self.initialized and self.parser.mtime():\n",
    "            allowed = self.parser.can_fetch(\"*\", url)\n",
    "\n",
    "        # Update cache\n",
    "        self.cache[cache_key] = {\n",
    "            'time': time.time(),\n",
    "            'allowed': allowed\n",
    "        }\n",
    "\n",
    "        return allowed\n",
    "\n",
    "class SitemapProcessor:\n",
    "    \"\"\"Enhanced sitemap processor with recursive support.\"\"\"\n",
    "    def __init__(self, recursive: bool = True):\n",
    "        self.recursive = recursive\n",
    "        self.processed_sitemaps = set()\n",
    "        self.max_retries = 3\n",
    "\n",
    "    async def process_sitemap(self, sitemap_url: str) -> Set[str]:\n",
    "        \"\"\"Process sitemap with support for index sitemaps.\"\"\"\n",
    "        urls = set()\n",
    "\n",
    "        if sitemap_url in self.processed_sitemaps:\n",
    "            return urls\n",
    "\n",
    "        self.processed_sitemaps.add(sitemap_url)\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    async with session.get(sitemap_url) as response:\n",
    "                        if response.status!= 200:\n",
    "                            break\n",
    "\n",
    "                        content = await response.text()\n",
    "\n",
    "                        # Try parsing as XML\n",
    "                        try:\n",
    "                            sitemap_dict = xmltodict.parse(content)\n",
    "                        except Exception:\n",
    "                            # Handle non-XML sitemaps (e.g., plain text)\n",
    "                            urls.update(\n",
    "                                url.strip() for url in content.splitlines()\n",
    "                                if url.strip().startswith('http')\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        # Process sitemap index\n",
    "                        if 'sitemapindex' in sitemap_dict:\n",
    "                            if self.recursive:\n",
    "                                sitemaps = sitemap_dict['sitemapindex']['sitemap']\n",
    "                                if isinstance(sitemaps, dict):\n",
    "                                    sitemaps = [sitemaps]\n",
    "\n",
    "                                for sitemap in sitemaps:\n",
    "                                    loc = sitemap.get('loc')\n",
    "                                    if loc:\n",
    "                                        sub_urls = await self.process_sitemap(loc)\n",
    "                                        urls.update(sub_urls)\n",
    "\n",
    "                        # Process urlset\n",
    "                        elif 'urlset' in sitemap_dict:\n",
    "                            url_entries = sitemap_dict['urlset']['url']\n",
    "                            if isinstance(url_entries, dict):\n",
    "                                url_entries = [url_entries]\n",
    "\n",
    "                            for entry in url_entries:\n",
    "                                loc = entry.get('loc')\n",
    "                                if loc:\n",
    "                                    urls.add(loc)\n",
    "\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        logger.warning(f\"Failed to process sitemap {sitemap_url}: {e}\")\n",
    "                    await asyncio.sleep(2 ** attempt)\n",
    "\n",
    "        return urls\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Advanced rate limiter with dynamic adjustment.\"\"\"\n",
    "    def __init__(self, min_delay: float, max_delay: float, max_concurrent: int):\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.current_delay = min_delay\n",
    "        self.last_request_time = defaultdict(float)\n",
    "        self.throttler = Throttler(rate_limit=max_concurrent)\n",
    "        self.response_times = deque(maxlen=100)\n",
    "        self.errors = deque(maxlen=100)\n",
    "\n",
    "    async def acquire(self, domain: str):\n",
    "        \"\"\"Acquire permission to make a request with dynamic rate limiting.\"\"\"\n",
    "        async with self.throttler:\n",
    "            # Calculate time since last request to this domain\n",
    "            elapsed = time.time() - self.last_request_time[domain]\n",
    "            if elapsed < self.current_delay:\n",
    "                await asyncio.sleep(self.current_delay - elapsed)\n",
    "\n",
    "            self.last_request_time[domain] = time.time()\n",
    "\n",
    "    def update_metrics(self, response_time: float, success: bool):\n",
    "        \"\"\"Update metrics to adjust rate limiting.\"\"\"\n",
    "        self.response_times.append(response_time)\n",
    "        self.errors.append(not success)\n",
    "\n",
    "        # Calculate error rate\n",
    "        error_rate = sum(self.errors) / len(self.errors) if self.errors else 0\n",
    "\n",
    "        # Adjust delay based on metrics\n",
    "        if error_rate > 0.1:  # More than 10% errors\n",
    "            self.current_delay = min(self.current_delay * 1.5, self.max_delay)\n",
    "        elif error_rate < 0.05 and len(self.response_times) > 0 and statistics.mean(self.response_times) < 1.0:\n",
    "            self.current_delay = max(self.current_delay * 0.8, self.min_delay)\n",
    "\n",
    "class SiteDumper:\n",
    "    \"\"\"Advanced web crawler with docling integration and real-time monitoring.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.stats = CrawlerStats()\n",
    "        self.path_filter = PathFilter(config.include_patterns, config.exclude_patterns)\n",
    "        self.content_processor = ContentProcessor(config.clean_output)\n",
    "        self.url_queue = URLQueue(config.max_depth)\n",
    "        self.rate_limiter = RateLimiter(\n",
    "            min_delay=config.request_delay_min,\n",
    "            max_delay=config.request_delay_max,\n",
    "            max_concurrent=config.max_concurrent\n",
    "        )\n",
    "\n",
    "        # Initialize components\n",
    "        self.robots = None if not config.obey_robots_txt else RobotsProcessor(str(config.website_url))\n",
    "        self.sitemap = SitemapProcessor(config.recursive_sitemap)\n",
    "\n",
    "        # Setup playwright\n",
    "        self.browser = None\n",
    "        self.browser_context = None\n",
    "\n",
    "        # Setup output directories\n",
    "        self.output_dir = Path(config.output_dir)\n",
    "        self.setup_directories()\n",
    "\n",
    "        # Configure logging\n",
    "        self.setup_logging()\n",
    "\n",
    "        # Initialize docling document collection\n",
    "        self.documents = []  # Initialize as an empty list to store processed documents\n",
    "\n",
    "\n",
    "        # Initialize API if enabled\n",
    "        self.api = None\n",
    "        if config.enable_api:\n",
    "            self.setup_api()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directory structure.\"\"\"\n",
    "        directories = [\n",
    "            self.output_dir,\n",
    "            *[self.output_dir / fmt for fmt in self.config.export_formats],\n",
    "            self.output_dir / \"logs\",\n",
    "            self.output_dir / \"stats\",\n",
    "            self.output_dir / \"media\",\n",
    "            self.output_dir / \"reports\",\n",
    "            self.output_dir / \"docling_models\"  # For docling document models\n",
    "        ]\n",
    "\n",
    "        for directory in directories:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure advanced logging.\"\"\"\n",
    "        log_format = (\n",
    "            \"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n",
    "            \"<level>{level: <8}</level> | \"\n",
    "            \"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | \"\n",
    "            \"<white>{message}</white>\"\n",
    "        )\n",
    "\n",
    "        logger.remove()\n",
    "        logger.add(\n",
    "            self.output_dir / \"logs/debug.log\",\n",
    "            format=log_format,\n",
    "            level=\"DEBUG\",\n",
    "            rotation=\"100 MB\",\n",
    "            compression=\"zip\"\n",
    "        )\n",
    "        logger.add(\n",
    "            self.output_dir / \"logs/error.log\",\n",
    "            format=log_format,\n",
    "            level=\"ERROR\",\n",
    "            rotation=\"100 MB\",\n",
    "            compression=\"zip\"\n",
    "        )\n",
    "\n",
    "    def setup_api(self):\n",
    "        \"\"\"Initialize FastAPI instance with enhanced endpoints.\"\"\"\n",
    "        self.api = FastAPI(\n",
    "            title=\"SiteDumper API\",\n",
    "            description=\"Real-time monitoring and control API for SiteDumper\",\n",
    "            version=\"2025.2.7\"\n",
    "        )\n",
    "\n",
    "        @self.api.get(\"/status\")\n",
    "        async def get_status():\n",
    "            \"\"\"Get the current status of the crawler.\"\"\"\n",
    "            return {\n",
    "                \"status\": \"running\",\n",
    "                \"stats\": self.stats.get_summary(),\n",
    "                \"queue\": self.url_queue.get_stats(),\n",
    "                \"memory\": {\n",
    "                    \"current\": self.stats.current_memory_usage,\n",
    "                    \"peak\": self.stats.peak_memory_usage\n",
    "                },\n",
    "                \"rate_limiting\": {\n",
    "                    \"current_delay\": self.rate_limiter.current_delay,\n",
    "                    \"error_rate\": sum(self.rate_limiter.errors) / len(self.rate_limiter.errors) if self.rate_limiter.errors else 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "        @self.api.get(\"/stats/documents\")\n",
    "        async def get_document_stats():\n",
    "            \"\"\"Get statistics about the processed documents.\"\"\"\n",
    "            return {\n",
    "                \"total_documents\": len(self.documents),\n",
    "                \"average_length\": statistics.mean([len(doc.text) for doc in self.documents]) if self.documents else 0,\n",
    "                \"content_types\": Counter(doc['content_type'] for doc in self.documents if 'content_type' in doc)\n",
    "            }\n",
    "\n",
    "\n",
    "        @self.api.post(\"/pause\")\n",
    "        async def pause_crawler():\n",
    "            \"\"\"Pause the crawler (not implemented yet).\"\"\"\n",
    "            raise HTTPException(status_code=501, detail=\"Not Implemented\")\n",
    "\n",
    "        @self.api.post(\"/resume\")\n",
    "        async def resume_crawler():\n",
    "            \"\"\"Resume the crawler (not implemented yet).\"\"\"\n",
    "            raise HTTPException(status_code=501, detail=\"Not Implemented\")\n",
    "\n",
    "\n",
    "    async def initialize_browser(self):\n",
    "        \"\"\"Initialize and configure browser instance.\"\"\"\n",
    "        playwright = await async_playwright().start()\n",
    "\n",
    "        browser_args = {\n",
    "            \"headless\": True,\n",
    "            \"proxy\": {\"server\": self.config.proxy_url} if self.config.use_proxy else None\n",
    "        }\n",
    "\n",
    "        self.browser = await playwright.chromium.launch(**browser_args)\n",
    "\n",
    "        context_args = {\n",
    "            \"viewport\": {\"width\": 1920, \"height\": 1080},\n",
    "            \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"  # Default, can be overridden by profile\n",
    "        }\n",
    "\n",
    "        if self.config.browser_profile == \"mobile\":\n",
    "            context_args[\"user_agent\"] = \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1\"\n",
    "        elif self.config.browser_profile == \"desktop\":\n",
    "            context_args[\"user_agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "        self.browser_context = await self.browser.new_context(**context_args)\n",
    "\n",
    "    async def process_page(self, url: str, depth: int) -> bool:\n",
    "        \"\"\"Process a single page with docling integration.\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "\n",
    "        async with self.rate_limiter.throttler:  # Use throttler directly\n",
    "            try:\n",
    "                await self.rate_limiter.acquire(domain)\n",
    "                start_time = time.time()\n",
    "\n",
    "                page = await self.browser_context.new_page()\n",
    "\n",
    "                try:\n",
    "                    # Configure page\n",
    "                    await page.set_extra_http_headers({\"Accept-Language\": \"en-US,en;q=0.9\"})\n",
    "\n",
    "                    # Navigate with retry logic\n",
    "                    for attempt in range(self.config.retry_attempts):\n",
    "                        try:\n",
    "                            response = await page.goto(\n",
    "                                url,\n",
    "                                wait_until=\"networkidle\",\n",
    "                                timeout=self.config.request_timeout * 1000\n",
    "                            )\n",
    "\n",
    "                            if response and response.ok:\n",
    "                                break\n",
    "\n",
    "                        except PlaywrightTimeoutError:\n",
    "                            logger.warning(f\"Timeout on attempt {attempt+1} for {url}\")\n",
    "                            if attempt == self.config.retry_attempts - 1:\n",
    "                                raise\n",
    "                            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "                        except Exception as e:\n",
    "                            if attempt == self.config.retry_attempts - 1:\n",
    "                                raise\n",
    "                            await asyncio.sleep(2 ** attempt)\n",
    "\n",
    "\n",
    "                    if not response or not response.ok:\n",
    "                        self.stats.add_error(url, f\"HTTP {response.status if response else 'No response'}\")\n",
    "                        return False\n",
    "\n",
    "                    # Get page content\n",
    "                    content = await page.content()\n",
    "\n",
    "                    # Process content with docling integration\n",
    "                    processed_content = await self.content_processor.process_content(\n",
    "                        url,\n",
    "                        content,\n",
    "                        dict(response.headers)\n",
    "                    )\n",
    "\n",
    "                    # Save content and document model\n",
    "                    await self.save_content(processed_content)\n",
    "                    self.documents.append(processed_content)  # Append the entire dictionary\n",
    "\n",
    "                    # Update statistics\n",
    "                    process_time = time.time() - start_time\n",
    "                    self.stats.add_page(\n",
    "                        url=url,\n",
    "                        size=len(content),\n",
    "                        content_type=response.headers.get(\"content-type\", \"unknown\"),\n",
    "                        status_code=response.status\n",
    "                    )\n",
    "                    self.stats.add_processing_time(process_time)\n",
    "\n",
    "                    # Update rate limiter metrics\n",
    "                    self.stats.add_download_time(process_time)  # Use process_time as a proxy for download time\n",
    "                    self.rate_limiter.update_metrics(process_time, True)\n",
    "\n",
    "                    # Process new URLs\n",
    "                    if not self.config.sitemap_only:\n",
    "                        for link_type, links in processed_content[\"links\"].items():\n",
    "                            if link_type in [\"internal\", \"external\"]:\n",
    "                                for link in links:\n",
    "                                    link_url = link[\"url\"]\n",
    "                                    if self.should_crawl_url(link_url):\n",
    "                                        await self.url_queue.add_url(\n",
    "                                            link_url,\n",
    "                                            depth + 1,\n",
    "                                            referrer=url\n",
    "                                        )\n",
    "\n",
    "                    return True\n",
    "\n",
    "                finally:\n",
    "                    await page.close()\n",
    "\n",
    "            except Exception as e:\n",
    "                self.stats.add_error(url, str(e))\n",
    "                self.rate_limiter.update_metrics(time.time() - start_time, False)\n",
    "                logger.exception(f\"Error processing {url}\")\n",
    "                return False\n",
    "\n",
    "    def should_crawl_url(self, url: str) -> bool:\n",
    "        \"\"\"Determine if a URL should be crawled based on configuration.\"\"\"\n",
    "        # Check if URL is already processed\n",
    "        if url in self.url_queue.processed_urls:\n",
    "            return False\n",
    "\n",
    "        # Check robots.txt if enabled\n",
    "        if self.robots and not self.robots.can_fetch(url):\n",
    "            self.stats.add_skipped(url, \"Disallowed by robots.txt\")\n",
    "            return False\n",
    "\n",
    "        # Check domain if stay_on_domain is enabled\n",
    "        if self.config.stay_on_domain:\n",
    "            base_domain = urlparse(str(self.config.website_url)).netloc\n",
    "            url_domain = urlparse(url).netloc\n",
    "            if base_domain!= url_domain:\n",
    "                self.stats.add_skipped(url, \"Outside of base domain\")\n",
    "                return False\n",
    "\n",
    "        # Check path filters\n",
    "        if not self.path_filter.should_process(url):\n",
    "            self.stats.add_skipped(url, \"Filtered by include/exclude patterns\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    async def save_content(self, processed_content: dict):\n",
    "        \"\"\"Save processed content to appropriate formats.\"\"\"\n",
    "        url = processed_content['url']\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path.strip('/')\n",
    "        if not path:\n",
    "          path = \"index\"\n",
    "\n",
    "        # Ensure the path is safe and doesn't contain invalid characters\n",
    "        path = re.sub(r'[\\\\/*?:\"<>|]', \"_\", path)\n",
    "\n",
    "        if self.config.maintain_hierarchy:\n",
    "            # Create directory structure based on URL hierarchy\n",
    "            base_path = self.output_dir / parsed_url.netloc\n",
    "            full_path = base_path / path\n",
    "        else:\n",
    "            full_path = self.output_dir / path\n",
    "\n",
    "        # Handle index files\n",
    "        if full_path.name == \"\" or full_path.name.startswith(\"index.\"):\n",
    "            if self.config.maintain_hierarchy:\n",
    "                full_path = full_path.parent / \"index\"\n",
    "            else:\n",
    "                full_path = full_path / \"index\"\n",
    "\n",
    "        for fmt in self.config.export_formats:\n",
    "            try:\n",
    "                if fmt == \"markdown\":\n",
    "                    filepath = full_path.with_suffix(\".md\")\n",
    "                    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        await f.write(self.convert_to_markdown(processed_content))\n",
    "\n",
    "                elif fmt == \"json\":\n",
    "                    filepath = full_path.with_suffix(\".json\")\n",
    "                    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:\n",
    "                         await f.write(json.dumps(processed_content, indent=2, ensure_ascii=False))\n",
    "\n",
    "                elif fmt == \"text\":\n",
    "                    filepath = full_path.with_suffix(\".txt\")\n",
    "                    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        await f.write(processed_content['text'])\n",
    "\n",
    "                elif fmt == \"html\":\n",
    "                    filepath = full_path.with_suffix(\".html\")\n",
    "                    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:\n",
    "                        await f.write(processed_content.get('raw_html', ''))  # Save raw HTML if available\n",
    "\n",
    "                if self.config.compress_output:\n",
    "                    # Compress the file\n",
    "                    compressed_filepath = filepath.with_suffix(filepath.suffix + \".gz\")\n",
    "                    async with aiofiles.open(filepath, 'rb') as f_in:\n",
    "                        async with aiofiles.open(compressed_filepath, 'wb') as f_out:\n",
    "                            await f_out.write(gzip.compress(await f_in.read()))\n",
    "                    # Optionally remove the uncompressed file\n",
    "                    os.remove(filepath)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving content in {fmt} format for {url}: {e}\")\n",
    "                self.stats.add_error(url, f\"Failed to save in {fmt} format\")\n",
    "\n",
    "    def convert_to_markdown(self, processed_content: dict) -> str:\n",
    "        \"\"\"Convert processed content to Markdown format.\"\"\"\n",
    "        md = \"\"\n",
    "        if self.config.export_metadata:\n",
    "            md += f\"# {processed_content.get('title', 'No Title')}\\n\\n\"\n",
    "            md += f\"**URL:** {processed_content['url']}\\n\\n\"\n",
    "            if 'meta_tags' in processed_content:\n",
    "                md += \"## Metadata\\n\\n\"\n",
    "                for key, value in processed_content['meta_tags'].items():\n",
    "                    md += f\"* **{key}:** {value}\\n\"\n",
    "                md += \"\\n\"\n",
    "\n",
    "        md += \"## Content\\n\\n\"\n",
    "        md += processed_content['text'] + \"\\n\\n\"\n",
    "\n",
    "        if 'links' in processed_content and self.config.export_metadata:\n",
    "            md += \"## Links\\n\\n\"\n",
    "            for link_type, links in processed_content['links'].items():\n",
    "                md += f\"### {link_type.capitalize()} Links\\n\\n\"\n",
    "                for link in links:\n",
    "                    md += f\"* [{link.get('text', link['url'])}]({link['url']})\\n\"\n",
    "            md += \"\\n\"\n",
    "\n",
    "        if 'images' in processed_content and self.config.export_metadata:\n",
    "            md += \"## Images\\n\\n\"\n",
    "            for image in processed_content['images']:\n",
    "                md += f\"!({image['url']})  \\n\"\n",
    "                if image['alt']:\n",
    "                    md+= f\"*{image['alt']}*  \\n\"\n",
    "                md += \"\\n\"\n",
    "\n",
    "        return md\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"Execute the crawler with enhanced monitoring.\"\"\"\n",
    "        try:\n",
    "            # Initialize browser\n",
    "            await self.initialize_browser()\n",
    "\n",
    "            # Start API if enabled in a separate task\n",
    "            if self.config.enable_api:\n",
    "                asyncio.create_task(uvicorn.run(self.api, host=\"0.0.0.0\", port=self.config.api_port))\n",
    "\n",
    "            # Initialize robots.txt if needed\n",
    "            if self.robots:\n",
    "                await self.robots.initialize()\n",
    "\n",
    "            # Process sitemap if enabled\n",
    "            if self.config.process_sitemaps:\n",
    "                sitemap_url = urljoin(str(self.config.website_url), \"/sitemap.xml\")\n",
    "                sitemap_urls = await self.sitemap.process_sitemap(sitemap_url)\n",
    "                for url in sitemap_urls:\n",
    "                    await self.url_queue.add_url(url)\n",
    "\n",
    "            # Add seed URL if not using sitemap only\n",
    "            if not self.config.sitemap_only:\n",
    "                await self.url_queue.add_url(str(self.config.website_url))\n",
    "\n",
    "            # Main crawling loop with enhanced progress visualization\n",
    "            with create_progress_bar() as progress:\n",
    "                task_id = progress.add_task(\n",
    "                    \"[cyan]Crawling...\",\n",
    "                    total=self.config.max_pages\n",
    "                )\n",
    "\n",
    "                while self.stats.pages_processed < self.config.max_pages and not self.url_queue.is_empty():\n",
    "                    url_data = await self.url_queue.get_next()\n",
    "                    if not url_data:\n",
    "                        break\n",
    "\n",
    "                    success = await self.process_page(url_data[\"url\"], url_data[\"depth\"])\n",
    "                    if success:\n",
    "                        progress.update(\n",
    "                            task_id,\n",
    "                            advance=1,\n",
    "                            refresh=True,\n",
    "                            memory=f\"{self.stats.current_memory_usage:.1f} MB\"  # Show current memory\n",
    "                        )\n",
    "\n",
    "                        # Generate interim report every 100 pages\n",
    "                        if self.stats.pages_processed % 100 == 0:\n",
    "                            await self.generate_interim_report()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Crawler stopped by user\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Fatal crawler error\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Cleanup and generate final report\n",
    "            if self.browser:\n",
    "                await self.browser.close()  # Properly close the browser\n",
    "            self.stats.finish()\n",
    "            await self.generate_final_report()\n",
    "\n",
    "            if self.config.enable_api:\n",
    "                logger.info(f\"API server running on port {self.config.api_port}\")\n",
    "\n",
    "\n",
    "    async def generate_interim_report(self):\n",
    "        \"\"\"Generate interim progress report.\"\"\"\n",
    "        report_path = self.output_dir / \"reports\" / f\"interim_report_{self.stats.pages_processed}.json\"\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"stats\": self.stats.get_summary(),\n",
    "            \"queue_status\": self.url_queue.get_stats(),\n",
    "            \"memory_usage\": {\n",
    "                \"current\": self.stats.curreent_memory_usage,\n",
    "                \"peak\": self.stats.peak_memory_usage\n",
    "            },\n",
    "            \"document_stats\": {\n",
    "                \"total_documents\": len(self.documents),\n",
    "                \"content_types\": Counter(doc['content_type'] for doc in self.documents if 'content_type' in doc)\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async with aiofiles.open(report_path, 'w') as f:\n",
    "            await f.write(json.dumps(report, indent=2))\n",
    "\n",
    "    async def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report.\"\"\"\n",
    "        report_path = self.output_dir / \"reports\" / \"final_report.json\"\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"configuration\": self.config.dict(),\n",
    "            \"stats\": self.stats.get_summary(),\n",
    "            \"document_stats\": {\n",
    "                \"total_documents\": len(self.documents),\n",
    "                \"content_types\": Counter(doc['content_type'] for doc in self.documents if 'content_type' in doc),\n",
    "                \"average_length\": statistics.mean([len(doc['text']) for doc in self.documents if 'text' in doc]) if self.documents else 0\n",
    "            },\n",
    "            \"error_summary\": dict(self.stats.errors),\n",
    "            \"performance_metrics\": {\n",
    "                \"average_processing_time\": statistics.mean(self.stats.processing_times) if self.stats.processing_times else 0,\n",
    "                \"peak_memory_usage\": self.stats.peak_memory_usage,\n",
    "                \"total_downloaded\": self.stats.bytes_downloaded\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async with aiofiles.open(report_path, 'w') as f:\n",
    "            await f.write(json.dumps(report, indent=2))\n",
    "\n",
    "    def create_progress_bar():\n",
    "        \"\"\"Creates and returns a rich Progress instance.\"\"\"\n",
    "        return Progress(\n",
    "            SpinnerColumn(),\n",
    "            TextColumn(\"[progress.description]{task.description}\"),\n",
    "            BarColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "            MofNCompleteColumn(),\n",
    "            TimeRemainingColumn(),\n",
    "            TimeElapsedColumn(),\n",
    "            TextColumn(\"• Mem: {task.fields[memory]}\"),  # Include memory usage\n",
    "            console=console,\n",
    "            transient=True,\n",
    "            refresh_per_second=10,\n",
    "        )\n",
    "\n",
    "\n",
    "#@title 🎯 Target Website Configuration {display-mode: \"form\"}\n",
    "website_url = \"https://markmap.js.org/docs\" #@param {type:\"string\"}\n",
    "max_pages = 1000 #@param {type:\"integer\"}\n",
    "max_depth = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "stay_on_domain = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@title 🤖 Compliance Settings {display-mode: \"form\"}\n",
    "obey_robots_txt = False #@param {type:\"boolean\"}\n",
    "process_sitemaps = True #@param {type:\"boolean\"}\n",
    "recursive_sitemap = True #@param {type:\"boolean\"}\n",
    "sitemap_only = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@title 🔍 Path Filtering {display-mode: \"form\"}\n",
    "#@markdown Enter patterns to include/exclude (comma-separated)\n",
    "include_patterns = \"\" #@param {type:\"string\"}\n",
    "exclude_patterns = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@title 🌐 Browser Configuration {display-mode: \"form\"}\n",
    "stealth_mode = \"Maximum\" #@param [\"Basic\", \"Moderate\", \"Maximum\"]\n",
    "browser_profile = \"random\" #@param [\"random\", \"mobile\", \"desktop\"]\n",
    "use_proxy = False #@param {type:\"boolean\"}\n",
    "proxy_url = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@title ⚡ Performance Settings {display-mode: \"form\"}\n",
    "#@markdown ### Request Timing\n",
    "request_delay_min = 1.0 #@param {type:\"slider\", min:0.5, max:5.0, step:0.5}\n",
    "request_delay_max = 3.0 #@param {type:\"slider\", min:1.0, max:10.0, step:0.5}\n",
    "request_timeout = 30 #@param {type:\"slider\", min:5, max:120, step:5}\n",
    "max_concurrent = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "retry_attempts = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
    "\n",
    "#@title 💾 Output Configuration {display-mode: \"form\"}\n",
    "#@markdown ### Export Format Selection\n",
    "export_markdown = True #@param {type:\"boolean\"}\n",
    "export_json = True #@param {type:\"boolean\"}\n",
    "export_text = True #@param {type:\"boolean\"}\n",
    "export_html = True #@param {type:\"boolean\"}\n",
    "output_dir = \"markmap docs site\" #@param {type:\"string\"}\n",
    "compress_output = True #@param {type:\"boolean\"}\n",
    "structure_output = True #@param {type:\"boolean\"}\n",
    "maintain_hierarchy = True #@param {type:\"boolean\"}\n",
    "clean_output = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@title 📊 Export Options {display-mode: \"form\"}\n",
    "export_metadata = True #@param {type:\"boolean\"}\n",
    "export_stats = True #@param {type:\"boolean\"}\n",
    "export_sitemap = True #@param {type:\"boolean\"}\n",
    "include_timestamps = True #@param {type:\"boolean\"}\n",
    "include_checksums = True #@param {type:\"boolean\"}\n",
    "generate_report = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@title 🔧 Advanced Settings {display-mode: \"form\"}\n",
    "enable_api = False #@param {type:\"boolean\"}\n",
    "api_port = 8000 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "def create_config_from_forms() -> Config:\n",
    "    \"\"\"Create Config object from form inputs.\"\"\"\n",
    "    # Initialize export formats list properly\n",
    "    export_formats = []\n",
    "\n",
    "    # Append selected formats based on form input\n",
    "    if export_markdown:\n",
    "        export_formats.append(\"markdown\")\n",
    "    if export_json:\n",
    "        export_formats.append(\"json\")\n",
    "    if export_text:\n",
    "        export_formats.append(\"text\")\n",
    "    if export_html:\n",
    "        export_formats.append(\"html\")\n",
    "\n",
    "    # Process include and exclude patterns\n",
    "    include_patterns_list = [p.strip() for p in include_patterns.split(\",\") if p.strip()]\n",
    "    exclude_patterns_list = [p.strip() for p in exclude_patterns.split(\",\") if p.strip()]\n",
    "\n",
    "    # Return configuration object\n",
    "    return Config(\n",
    "        user_login=USER_LOGIN,\n",
    "        current_utc=CURRENT_UTC,\n",
    "        website_url=website_url,\n",
    "        max_pages=max_pages,\n",
    "        max_depth=max_depth,\n",
    "        stay_on_domain=stay_on_domain,\n",
    "        obey_robots_txt=obey_robots_txt,\n",
    "        process_sitemaps=process_sitemaps,\n",
    "        recursive_sitemap=recursive_sitemap,\n",
    "        sitemap_only=sitemap_only,\n",
    "        include_patterns=include_patterns_list,\n",
    "        exclude_patterns=exclude_patterns_list,\n",
    "        stealth_mode=stealth_mode,\n",
    "        browser_profile=browser_profile,\n",
    "        use_proxy=use_proxy,\n",
    "        proxy_url=proxy_url,\n",
    "        request_delay_min=request_delay_min,\n",
    "        request_delay_max=request_delay_max,\n",
    "        request_timeout=request_timeout,\n",
    "        max_concurrent=max_concurrent,\n",
    "        retry_attempts=retry_attempts,\n",
    "        export_formats=export_formats,\n",
    "        output_dir=output_dir,\n",
    "        compress_output=compress_output,\n",
    "        structure_output=structure_output,\n",
    "        maintain_hierarchy=maintain_hierarchy,\n",
    "        clean_output=clean_output,\n",
    "        export_metadata=export_metadata,\n",
    "        export_stats=export_stats,\n",
    "        export_sitemap=export_sitemap,\n",
    "        include_timestamps=include_timestamps,\n",
    "        include_checksums=include_checksums,\n",
    "        generate_report=generate_report,\n",
    "        enable_api=enable_api,\n",
    "        api_port=api_port\n",
    "    )\n",
    "\n",
    "\n",
    "# Create configuration from form inputs and display summary\n",
    "config = create_config_from_forms()\n",
    "\n",
    "console = Console(\n",
    "    color_system=\"truecolor\",\n",
    "    width=100,\n",
    "    style=Style(color=\"cyan\", bgcolor=None),\n",
    "    highlight=True,\n",
    "    record=True,\n",
    "    markup=True\n",
    ")\n",
    "\n",
    "console.print(Panel(\n",
    "    \"\\n\".join([\n",
    "        f\"[cyan]Configuration Summary[/cyan]\",\n",
    "        f\"Target URL: {config.website_url}\",\n",
    "        f\"Max Pages: {config.max_pages}\",\n",
    "        f\"Max Depth: {config.max_depth}\",\n",
    "        f\"Output Directory: {config.output_dir}\",\n",
    "        f\"Export Formats: {', '.join(config.export_formats)}\",\n",
    "        f\"Browser Profile: {config.browser_profile}\",\n",
    "        f\"Stealth Mode: {config.stealth_mode}\"\n",
    "    ]),\n",
    "    title=\"SiteDumper Configuration\",\n",
    "    border_style=\"cyan\",\n",
    "    padding=(1, 2)\n",
    "))\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main execution flow.\"\"\"\n",
    "    try:\n",
    "        dumper = SiteDumper(config)\n",
    "\n",
    "        # Display startup banner\n",
    "        console.print(Panel(\n",
    "            f\"[cyan]SiteDumper v2025.2.7[/cyan]\\n\" +\n",
    "            f\"User: {config.user_login}\\n\" +\n",
    "            f\"Started at: {config.current_utc}\\n\" +\n",
    "            f\"Target: {config.website_url}\\n\" +\n",
    "            f\"Output: {config.output_dir}\",\n",
    "            title=\"Crawler Status\",\n",
    "            border_style=\"cyan\",\n",
    "            padding=(1, 2)\n",
    "        ))\n",
    "\n",
    "        await dumper.run()\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]Fatal Error:[/red] {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    uvloop.install()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkhfo_Ckoppo"
   },
   "outputs": [],
   "source": [
    " import shutil\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"\", topdown=False):\n",
    "    for dir in dirs:\n",
    "        dir_path = os.path.join(root, dir)\n",
    "        try:\n",
    "            shutil.rmtree(dir_path)  # Use shutil.rmtree to remove non-empty dirs\n",
    "            print(f\"Deleted directory: {dir_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting directory {dir_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WcOxdHOsRRP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNk8llrrC/XJG8wpB4WwkbP",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
